{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GhBlg/Others/blob/main/sweeps_with_blocks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pceY6sGNaTvE"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f2tZmmCFaLLz",
        "outputId": "41ee5556-3dbe-46ac-8cbc-f82f14d3776f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 71.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 89.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 89.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 75.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=30362db1454fafde6b58b4e0839fe8336d7d4dab3372594a6f337124bdbbefa7\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.1 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.21.6)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "Using matplotlib backend: agg\n",
            "Mounted at /content/drive/\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting braindecode\n",
            "  Downloading Braindecode-0.6-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting skorch\n",
            "  Downloading skorch-0.11.0-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 62.0 MB/s \n",
            "\u001b[?25hCollecting mne\n",
            "  Downloading mne-1.1.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5 MB 33.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from braindecode) (3.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.7.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from braindecode) (3.2.2)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->braindecode) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->braindecode) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->braindecode) (1.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne->braindecode) (21.3)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne->braindecode) (1.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne->braindecode) (2.11.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne->braindecode) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne->braindecode) (4.64.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne->braindecode) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne->braindecode) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->braindecode) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->braindecode) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->braindecode) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne->braindecode) (2022.6.15)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne->braindecode) (2.0.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->braindecode) (2022.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch->braindecode) (0.8.10)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch->braindecode) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch->braindecode) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch->braindecode) (3.1.0)\n",
            "Installing collected packages: skorch, mne, braindecode\n",
            "Successfully installed braindecode-0.6 mne-1.1.0 skorch-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from mne) (4.64.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne) (21.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from mne) (3.2.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.7/dist-packages (from mne) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from mne) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne) (2.11.3)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from mne) (1.21.6)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.5->mne) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->mne) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->mne) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->mne) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->mne) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting moabb\n",
            "  Downloading moabb-0.4.6-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pooch<2.0,>=1.6 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.6.0)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.0.2)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (3.2.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.21.6)\n",
            "Requirement already satisfied: seaborn>=0.9 in /usr/local/lib/python3.7/dist-packages (from moabb) (0.11.2)\n",
            "Requirement already satisfied: h5py<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (3.1.0)\n",
            "Collecting pyriemann>=0.2.6\n",
            "  Downloading pyriemann-0.3.tar.gz (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 49.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.15.1 in /usr/local/lib/python3.7/dist-packages (from moabb) (2.23.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.62 in /usr/local/lib/python3.7/dist-packages (from moabb) (4.64.0)\n",
            "Requirement already satisfied: mne>=0.19 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.1.0)\n",
            "Requirement already satisfied: scipy<2.0,>=1.5 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.7.3)\n",
            "Collecting PyYAML<6.0,>=5.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 77.9 MB/s \n",
            "\u001b[?25hCollecting coverage<6.0,>=5.5\n",
            "  Downloading coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 87.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py<4.0,>=3.0->moabb) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->moabb) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->moabb) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->moabb) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->moabb) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4.0,>=3.0->moabb) (4.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from mne>=0.19->moabb) (4.4.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from mne>=0.19->moabb) (21.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from mne>=0.19->moabb) (2.11.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0->moabb) (2022.1)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch<2.0,>=1.6->moabb) (1.4.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyriemann>=0.2.6->moabb) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0,>=3.0->moabb) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (1.24.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0,>=1.0->moabb) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->mne>=0.19->moabb) (2.0.1)\n",
            "Building wheels for collected packages: pyriemann\n",
            "  Building wheel for pyriemann (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyriemann: filename=pyriemann-0.3-py2.py3-none-any.whl size=78033 sha256=b8133a3a77b4e738458290f900df157dd3ddfccb9a3a993ee9cd0f36cc9b1647\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/1b/bf/a537f9e17e6c3490004ede419c72f863af1d0d765d25e532ef\n",
            "Successfully built pyriemann\n",
            "Installing collected packages: PyYAML, pyriemann, coverage, moabb\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 coverage-5.5 moabb-0.4.6 pyriemann-0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Will use device cuda\n",
            "Will save checkpoints to /content/resultdir\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "!pip install scipy -U\n",
        "!wandb login acf4cf1e91a5b92ebf1613195d6d05c09db63b4e\n",
        "import math\n",
        "import numpy as np\n",
        "from math import ceil\n",
        "import wandb\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.nn.utils import weight_norm\n",
        "%matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/Colab Notebooks/mne_data')\n",
        "sys.path.append(os.path.abspath('/content/drive/MyDrive/Colab Notebooks/mne_data'))\n",
        "!mkdir /content/resultdir\n",
        "!pip install torchinfo\n",
        "from torchinfo import summary\n",
        "resultdir='/content/resultdir'\n",
        "!pip install braindecode\n",
        "!pip install tqdm\n",
        "!pip install mne\n",
        "!pip install moabb\n",
        "\n",
        "###########     REPRODUCIBILITY      #######################################\n",
        "random_seed=42\n",
        "#torch.use_deterministic_algorithms(True)\n",
        "torch.manual_seed(random_seed)\n",
        "############################################################################\n",
        "\n",
        "############################################################################\n",
        "\n",
        "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it (will take the gpu specified by the argument args.device)\n",
        "device = 'cuda' if cuda else 'cpu'\n",
        "os.makedirs(resultdir,exist_ok=True)\n",
        "print(f\"Will use device {device}\")\n",
        "print(f\"Will save checkpoints to {resultdir}\")\n",
        "################################################################################################################################# \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HsHADplayNq"
      },
      "source": [
        "###Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i2K62OZWaxy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f91f9dc-7753-4b14-9c39-cefbe4ea03eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/braindecode/datautil/windowers.py:4: UserWarning: datautil.windowers module is deprecated and is now under preprocessing.windowers, please use from import braindecode.preprocessing.windowers\n",
            "  warn('datautil.windowers module is deprecated and is now under '\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "# Import necessary library\n",
        "from scipy.linalg import sqrtm, inv \n",
        "import numpy as np\n",
        "from braindecode.datasets.moabb import MOABBDataset\n",
        "from braindecode.datautil.windowers import create_windows_from_events\n",
        "\n",
        "\n",
        "# Apply Euclidean Alignment\n",
        "def apply_EA(data, device='cuda'):\n",
        "    '''\n",
        "    Apply Euclidean aligment on array-like objects for 1 subject\n",
        "    \n",
        "    PARAMETER:\n",
        "    data: \n",
        "        Data of one subject.\n",
        "    \n",
        "    \n",
        "    OUTPUT:\n",
        "        Aligned data with Euclidean Alignment\n",
        "    '''\n",
        "    \n",
        "    # So that this function can handles separated or combined left and right trials\n",
        "    # If they are separated\n",
        "    # If they are not separated\n",
        "\n",
        "    #if input is a torch tensor\n",
        "    if torch.is_tensor(data):\n",
        "        data=data.cpu().detach().numpy() \n",
        "\n",
        "    print('Found %d trial(s) in which EEG data is stored' %len(data))\n",
        "    all_trials = data\n",
        "    \n",
        "    # Calculate reference matrix\n",
        "    RefEA = 0\n",
        "    print('Computing reference matrix RefEA')\n",
        "\n",
        "    # Iterate over all trials, compute reference EA\n",
        "    for trial in all_trials:\n",
        "        cov = np.cov(trial, rowvar=True)\n",
        "        RefEA += cov\n",
        "\n",
        "    # Average over all trials\n",
        "    RefEA = RefEA/all_trials.shape[0]\n",
        "    \n",
        "    # Adding reference EA as a new key in data\n",
        "    data_dict={}\n",
        "    print('Add RefEA as a new key in data')\n",
        "    data_dict['RefEA'] = RefEA \n",
        "    \n",
        "    # Compute R^(-0.5)\n",
        "    R_inv = sqrtm(inv(RefEA))\n",
        "    data_dict['R_inv'] = R_inv\n",
        "    \n",
        "        \n",
        "    # Perform EA on each trial\n",
        "    all_trials_EA = []\n",
        "        \n",
        "    for t in all_trials:\n",
        "        all_trials_EA.append(R_inv@t)\n",
        "        \n",
        "    # Return all_trials_EA\n",
        "    return torch.tensor(np.array(all_trials_EA)).float()\n",
        "        \n",
        "\n",
        "def standardize(X, mean=None, std=None):\n",
        "  mean = X.mean(dim=-1, keepdim=True)\n",
        "  std = X.std(dim=-1, keepdim=True)\n",
        "  return (X - mean) / std , mean, std\n",
        "\n",
        "\n",
        "\n",
        "################### Load Physionet data as done in the TIDNet paper ###################################################################\n",
        "################## calling load_eeg_bci will return an object having data[subjects][records][0/1] 0 for Xs and 1 for Ys ###############\n",
        "from torch.utils.data import Dataset\n",
        "from mne.datasets import eegbci\n",
        "import mne \n",
        "import tqdm\n",
        "from collections import OrderedDict\n",
        "\n",
        "BAD_SUBJECTS_EEGBCI = [87, 89, 91, 99]\n",
        "SUBJECTS_EEGBCI = list(i for i in range(109) if i not in BAD_SUBJECTS_EEGBCI)\n",
        "EVENTS_EEGBCI = dict(hands=2, feet=3)\n",
        "BASELINE_EYES_OPEN = [1]\n",
        "BASELINE_EYES_CLOSED = [2]\n",
        "\n",
        "MOTOR_FISTS = (3, 7, 11)\n",
        "IMAGERY_FISTS = (4, 8, 12)\n",
        "MOTOR_FEET = (5, 9, 13)\n",
        "IMAGERY_FEET_V_FISTS = (6, 10, 14)\n",
        "\n",
        "def zscore(data: np.ndarray, axis=-1):\n",
        "    return (data - data.mean(axis, keepdims=True)) / (data.std(axis, keepdims=True) + 1e-12)\n",
        "\n",
        "\n",
        "def one_hot(y: torch.Tensor, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor to another similarly stored tensor\"\"\"\n",
        "    if len(y.shape) > 0 and y.shape[-1] == 1:\n",
        "        y = y.squeeze(-1)\n",
        "    out = torch.zeros(y.size()+torch.Size([num_classes]), device=y.device)\n",
        "    return out.scatter_(-1, y.view((*y.size(), 1)), 1)\n",
        "\n",
        "class EpochsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, epochs: mne.Epochs, force_label=None, picks=None, preproccesors=None, normalizer=zscore,\n",
        "                 runs=None, train_mode=False):\n",
        "        self.mode = train_mode\n",
        "        self.epochs = epochs\n",
        "        self._t_len = epochs.tmax - epochs.tmin\n",
        "        self.loaded_x = [None for _ in range(len(epochs.events))]\n",
        "        self.runs = runs\n",
        "        self.picks = picks\n",
        "        self.force_label = force_label if force_label is None else torch.tensor(force_label)\n",
        "        self.normalizer = normalizer\n",
        "        self.preprocessors = preproccesors if isinstance(preproccesors, (list, tuple)) else [preproccesors]\n",
        "        for i, p in enumerate(self.preprocessors):\n",
        "            self.preprocessors[i] = p(self.epochs)\n",
        "\n",
        "    @property\n",
        "    def channels(self):\n",
        "        if self.picks is None:\n",
        "            return len(self.epochs.ch_names)\n",
        "        else:\n",
        "            return len(self.picks)\n",
        "\n",
        "    @property\n",
        "    def sfreq(self):\n",
        "        return self.epochs.info['sfreq']\n",
        "\n",
        "    def train_mode(self, mode=False):\n",
        "        self.mode = mode\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ep = self.epochs[index]\n",
        "        if self.loaded_x[index] is None:\n",
        "            x = ep.get_data()\n",
        "            if len(x.shape) != 3 or 0 in x.shape:\n",
        "                print(\"I don't know why: {} index{}/{}\".format(self.epochs, index, len(self)))\n",
        "                print(self.epochs.info['description'])\n",
        "                # raise AttributeError()\n",
        "                return self.__getitem__(index - 1)\n",
        "            x = x[0, self.picks, :]\n",
        "            for p in self.preprocessors:\n",
        "                x = p(x)\n",
        "            x = torch.from_numpy(self.normalizer(x).astype('float32')).squeeze(0)\n",
        "            self.loaded_x[index] = x\n",
        "        else:\n",
        "            x = self.loaded_x[index]\n",
        "\n",
        "        y = torch.from_numpy(ep.events[..., -1]).long() if self.force_label is None else self.force_label\n",
        "\n",
        "        if self.runs is not None:\n",
        "            return x, y, one_hot(torch.tensor(self.runs * index / len(self)).long(), self.runs)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        events = self.epochs.events[:, 0].tolist()\n",
        "        return len(events)\n",
        "\n",
        "def same(x):\n",
        "    return x\n",
        "\n",
        "def load_eeg_bci(targets=4, tmin=0, tlen=3, t_ev=0, t_sub=None, normalizer=same, low_f=None, high_f=None, #zscore\n",
        "                 alignment=False, path_mne=None):\n",
        "\n",
        "    paths = [eegbci.load_data(s+1, IMAGERY_FISTS, path=path_mne, update_path=False) for s in SUBJECTS_EEGBCI]\n",
        "    raws = [mne.io.concatenate_raws([mne.io.read_raw_edf(p, preload=True) for p in path])\n",
        "            for path in tqdm.tqdm(paths, unit='subj', desc='Loading')]\n",
        "    datasets = OrderedDict()\n",
        "    for i, raw in tqdm.tqdm(list(zip(SUBJECTS_EEGBCI, raws)), desc='Preprocessing'):\n",
        "        if raw.info['sfreq'] != 160:\n",
        "            tqdm.tqdm.write('Skipping..., sampling frequency: {}'.format(raw.info['sfreq']))\n",
        "            continue\n",
        "        raw.rename_channels(lambda x: x.strip('.'))\n",
        "        if low_f or high_f:\n",
        "            raw.filter(low_f, high_f, fir_design='firwin', skip_by_annotation='edge')\n",
        "        events, _ = mne.events_from_annotations(raw, event_id=dict(T1=0, T2=1))\n",
        "        picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude='bads')\n",
        "        epochs = mne.Epochs(raw, events[:41, ...], tmin=tmin, tmax=tmin + tlen - 1 / raw.info['sfreq'], picks=picks,\n",
        "                            baseline=None, reject_by_annotation=False)#.drop_bad()\n",
        "        if targets > 2:\n",
        "            paths = eegbci.load_data(i + 1, BASELINE_EYES_OPEN, path=path_mne, update_path=False)\n",
        "            raw = mne.io.concatenate_raws([mne.io.read_raw_edf(p, preload=True) for p in paths])\n",
        "            raw.rename_channels(lambda x: x.strip('.'))\n",
        "            if low_f or high_f:\n",
        "                raw.filter(low_f, high_f, fir_design='firwin', skip_by_annotation='edge')\n",
        "            events = np.zeros((events.shape[0] // 2, 3)).astype('int')\n",
        "            events[:, -1] = 2\n",
        "            events[:, 0] = np.linspace(0, raw.info['sfreq'] * (60 - 2 * tlen), num=events.shape[0]).astype(np.int)\n",
        "            picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude='bads')\n",
        "            eyes_epochs = mne.Epochs(raw, events, tmin=tmin, tmax=tmin + tlen - 1 / raw.info['sfreq'], picks=picks,\n",
        "                                     baseline=None, reject_by_annotation=False)#.drop_bad()\n",
        "            epochs = mne.concatenate_epochs([eyes_epochs, epochs])\n",
        "        if targets > 3:\n",
        "            paths = eegbci.load_data(i+1, IMAGERY_FEET_V_FISTS, path=path_mne, update_path=False)\n",
        "            raw = mne.io.concatenate_raws([mne.io.read_raw_edf(p, preload=True) for p in paths])\n",
        "            raw.rename_channels(lambda x: x.strip('.'))\n",
        "            if low_f or high_f:\n",
        "                raw.filter(low_f, high_f, fir_design='firwin', skip_by_annotation='edge')\n",
        "            events, _ = mne.events_from_annotations(raw, event_id=dict(T2=3))\n",
        "            picks = mne.pick_types(raw.info, meg=False, eeg=True, stim=False, eog=False, exclude='bads')\n",
        "            feet_epochs = mne.Epochs(raw, events[:20, ...], tmin=tmin, tmax=tmin + tlen - 1 / raw.info['sfreq'],\n",
        "                                     picks=picks, baseline=None, reject_by_annotation=False)#.drop_bad()\n",
        "            epochs = mne.concatenate_epochs([epochs, feet_epochs])\n",
        "\n",
        "        datasets[i] = EpochsDataset(epochs, preproccesors=EuclideanAlignment if alignment else [],\n",
        "                                    normalizer=normalizer, runs=3)\n",
        "\n",
        "    return datasets\n",
        "\n",
        "\n",
        "####################################################################################################################\n",
        "\n",
        "\n",
        "\n",
        "def load_subjects(path, number_of_subjects, device, bad_subjects=[], apply_euclidean=True, with_eog=True):\n",
        "    l= number_of_subjects+1\n",
        "    sbj_x=[]\n",
        "    sbj_y=[]\n",
        "    if number_of_subjects==9:\n",
        "        for subject_id in [e for e in range(1,l) if e not in bad_subjects]:\n",
        "            dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])\n",
        "\n",
        "\n",
        "            trial_start_offset_seconds = -0.5\n",
        "            # Extract sampling frequency, check that they are same in all datasets\n",
        "            sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "            assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
        "            # Calculate the trial start offset in samples.\n",
        "            trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
        "\n",
        "            # Create windows using braindecode function for this. It needs parameters to define how\n",
        "            # trials should be used.\n",
        "            windows_dataset = create_windows_from_events(\n",
        "                dataset,\n",
        "                trial_start_offset_samples=trial_start_offset_samples,\n",
        "                trial_stop_offset_samples=0,\n",
        "                preload=True,\n",
        "            )\n",
        "\n",
        "\n",
        "            splitted = windows_dataset.split('session')\n",
        "            train_set = splitted['session_T']\n",
        "            valid_set = splitted['session_E']\n",
        "\n",
        "            # delete stim channel and eog channels if wanted\n",
        "            if with_eog==False: \n",
        "              dlt=-4\n",
        "            else:\n",
        "              dlt=-1\n",
        "\n",
        "            train_x=np.array([ele[0][:dlt] for ele in train_set])\n",
        "            train_y=np.array([ele[1] for ele in train_set])\n",
        "\n",
        "            valid_x=np.array([ele[0][:dlt] for ele in valid_set])\n",
        "            valid_y=np.array([ele[1] for ele in valid_set])\n",
        "\n",
        "            T_x = torch.tensor( np.append(np.array(train_x), np.array(valid_x), axis=0) )\n",
        "            T_y = torch.tensor( np.append(np.array(train_y), np.array(valid_y), axis=0) )\n",
        "\n",
        "            if apply_euclidean==True:\n",
        "                x=apply_EA(T_x)\n",
        "            else : \n",
        "                x=T_x\n",
        "\n",
        "            sbj_x.append(x)\n",
        "            sbj_y.append(T_y)\n",
        "            del T_x, T_y\n",
        "\n",
        "    elif number_of_subjects==109:\n",
        "        bad_subjects=[s-1 for s in bad_subjects]\n",
        "        data=load_eeg_bci(path_mne=path)\n",
        "        for subject_id in [e for e in range(number_of_subjects) if e not in bad_subjects]:   \n",
        "            xx=[]\n",
        "            yy=[]\n",
        "            for records in range(len(data[subject_id])):\n",
        "                xx.append(data[subject_id][records][0])\n",
        "                yy.append(data[subject_id][records][1])\n",
        "            xx=torch.stack(xx)\n",
        "            yy=torch.tensor(yy)\n",
        "            if apply_euclidean==True:\n",
        "                sbj_x.append(apply_EA(xx))\n",
        "            else:\n",
        "                sbj_x.append(xx)\n",
        "            sbj_y.append(yy)\n",
        "\n",
        "    return sbj_x, sbj_y\n",
        "\n",
        "\n",
        "\n",
        "#################### Leave One Subject Out ############################\n",
        "def loso(x, y, number_of_subjects, loso, device, bad_subjects=[], with_validation=True): \n",
        "    l= number_of_subjects\n",
        "    [s-1 for s in bad_subjects]\n",
        "    T_x=torch.tensor([])\n",
        "    T_y=torch.tensor([])\n",
        "\n",
        "    loso=loso-1 # because lists start from 0\n",
        "\n",
        "    for subject_id in [e for e in range(l) if e not in [loso]+bad_subjects]:\n",
        "        T_x=torch.cat((T_x,x[subject_id]), 0)\n",
        "        T_y=torch.cat((T_y,y[subject_id]), 0)\n",
        "\n",
        "    T_x=torch.tensor(T_x).reshape([-1, T_x.shape[1], T_x.shape[2]])\n",
        "    T_y=torch.tensor(T_y).reshape([-1])\n",
        "\n",
        "    k_f=int(0.9*T_x.shape[0])\n",
        "\n",
        "    if with_validation==True:\n",
        "        data_perm = torch.randperm(T_x.shape[0])\n",
        "        temp_x, temp_y = T_x[data_perm[:]], T_y[data_perm[:]]\n",
        "        T_x, T_y = temp_x[:k_f], temp_y[:k_f]\n",
        "        V_x, V_y = temp_x[k_f:], temp_y[k_f:]\n",
        "    else :\n",
        "        V_x, V_y = torch.tensor([]), torch.tensor([])\n",
        "\n",
        "\n",
        "    Test_x=x[loso]\n",
        "    Test_y=y[loso]\n",
        "\n",
        "    \n",
        "    T_x, mean, std=standardize(T_x)\n",
        "    if with_validation==True:\n",
        "        V_x, _ , _=standardize(V_x,mean, std)\n",
        "    Test_x, _ , _=standardize(Test_x, mean, std)\n",
        "\n",
        "    ############################################################################\n",
        "    return(T_x.to(device),T_y.to(device).to(torch.int64),V_x.to(device),V_y.to(device).to(torch.int64),Test_x.to(device),Test_y.to(device).to(torch.int64))\n",
        "\n",
        "\n",
        "#################### Leave Multiple Subjects Out ############################\n",
        "import random\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def lmso(x, y, number_of_subjects, kfold, device, bad_subjects=[], with_validation=True, random_seed=42): \n",
        "    c = list(zip(x, y))\n",
        "    random.Random(random_seed).shuffle(c)\n",
        "    x, y = zip(*c)\n",
        "    \n",
        "    # 10-fold crossvalidation\n",
        "    kf = KFold(n_splits=10)\n",
        "\n",
        "    x=np.array([t.numpy() for t in x])\n",
        "    y=np.array([t.numpy() for t in y])\n",
        "\n",
        "\n",
        "    i=1\n",
        "    for train_index, test_index in kf.split(x):\n",
        "        X_train, X_test = x[train_index], x[test_index]\n",
        "        y_train, y_test = y[train_index], y[test_index]\n",
        "        if i==kfold:\n",
        "            break\n",
        "        else:\n",
        "            i+=1\n",
        "    \n",
        "\n",
        "    del c, kf\n",
        "\n",
        "    l= number_of_subjects\n",
        "    [s-1 for s in bad_subjects]\n",
        "\n",
        "    T_x=torch.tensor([])\n",
        "    T_y=torch.tensor([])\n",
        "    Test_x=torch.tensor([])\n",
        "    Test_y=torch.tensor([])\n",
        "\n",
        "    for subject_id in [e for e in range(X_train.shape[0])]:\n",
        "        T_x=torch.cat((T_x,torch.tensor(X_train[subject_id])), 0)\n",
        "        T_y=torch.cat((T_y,torch.tensor(y_train[subject_id])), 0)\n",
        "\n",
        "    T_x=torch.tensor(T_x).reshape([-1, T_x.shape[1], T_x.shape[2]])\n",
        "    T_y=torch.tensor(T_y).reshape([-1])\n",
        "\n",
        "    k_f=int(0.9*T_x.shape[0])\n",
        "\n",
        "    if with_validation==True:\n",
        "        data_perm = torch.randperm(T_x.shape[0])\n",
        "        temp_x, temp_y = T_x[data_perm[:]], T_y[data_perm[:]]\n",
        "        T_x, T_y = temp_x[:k_f], temp_y[:k_f]\n",
        "        V_x, V_y = temp_x[k_f:], temp_y[k_f:]\n",
        "    else :\n",
        "        V_x, V_y = torch.tensor([]), torch.tensor([])\n",
        "\n",
        "    for lms in range(len(X_test)):\n",
        "        Test_x=torch.cat((Test_x,torch.tensor(X_test[lms])), 0)\n",
        "        Test_y=torch.cat((Test_y,torch.tensor(y_test[lms])), 0)\n",
        "\n",
        "    Test_x=torch.tensor(Test_x).reshape([-1, Test_x.shape[1], Test_x.shape[2]])\n",
        "    Test_y=torch.tensor(Test_y).reshape([-1])\n",
        "    \n",
        "    T_x, mean, std=standardize(T_x)\n",
        "    if with_validation==True:\n",
        "        V_x, _ , _=standardize(V_x,mean, std)\n",
        "    Test_x, _ , _=standardize(Test_x, mean, std)\n",
        "    \n",
        "    ############################################################################\n",
        "    return(T_x.to(device),T_y.to(device).to(torch.int64),V_x.to(device),V_y.to(device).to(torch.int64),Test_x.to(device),Test_y.to(device).to(torch.int64))\n",
        "\n",
        "\n",
        "#############Data Augmentation ###########################################################\n",
        "from scipy.signal import butter, lfilter\n",
        "    \n",
        "#flip channels\n",
        "def flip_channels(data):\n",
        "  r=torch.randperm(data.shape[1])\n",
        "  data=data[:,r,:]\n",
        "  return data\n",
        "\n",
        "#time inverse\n",
        "def time_inverse(data):\n",
        "  return torch.flip(data, [2])\n",
        "\n",
        "#Masking in %\n",
        "def masking(data,p):\n",
        "  x=data\n",
        "  mask=torch.FloatTensor(x.shape).uniform_() > p\n",
        "  masked_output = data * mask.int().float().to(device)\n",
        "  return masked_output\n",
        "\n",
        "\n",
        "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
        "    return butter(order, [lowcut, highcut], fs=fs, btype='band')\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y\n",
        "\n",
        "selection=['fc','ti','mask','filter']\n",
        "def apply_da(data,selection):\n",
        "  s=random.choice(selection)\n",
        "  if s=='fc':\n",
        "    data=flip_channels(data)\n",
        "  elif s=='ti':\n",
        "    data=time_inverse(data)\n",
        "  elif s=='mask':\n",
        "    data=masking(data,0.2)\n",
        "  elif s=='filter':\n",
        "    data=torch.tensor(butter_bandpass_filter(data.detach().cpu().numpy(),0.1, 40, 250)).float().to(device)\n",
        "  return data\n",
        "\n",
        "#####################################################################################\n",
        "\n",
        "def weight_attack(model, p=0.1):\n",
        "  ii=0\n",
        "  for i in model:\n",
        "    try:\n",
        "      r1=torch.max(model[ii].weight)\n",
        "      r2=torch.min(model[ii].weight)\n",
        "      M1=(r1 - r2) * torch.rand(model[ii].weight.shape)+ r2\n",
        "      M2=(torch.abs(M1)< p*(r1 - r2)).float()\n",
        "      with torch.no_grad():\n",
        "        M1=torch.flatten(M1)\n",
        "        M2=torch.flatten(M2)\n",
        "        mw=torch.flatten(model[2].weight)\n",
        "        shp=model[ii].weight.shape\n",
        "        for i in range(len(M1)):\n",
        "          if M2[i]==1:\n",
        "            mw[i]=M1[i]\n",
        "        mw=mw.double()\n",
        "\n",
        "        model[ii].weight=torch.nn.Parameter(mw.reshape(shp))\n",
        "    except:\n",
        "      pass\n",
        "    ii+=1\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egbHk-mvxavP"
      },
      "source": [
        "###coatnets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJS7JUooxdMO"
      },
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils import weight_norm\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "##############  EEG_CoatNet models #############################################################\n",
        "\n",
        "class PrintLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintLayer, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Do your print / debug stuff here\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "class Ensure4d(nn.Module):\n",
        "    def forward(self, x):\n",
        "        while(len(x.shape) < 4):\n",
        "            x = x.unsqueeze(-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Expression(nn.Module):\n",
        "    \"\"\"Compute given expression on forward pass.\n",
        "    Parameters\n",
        "    ----------\n",
        "    expression_fn : callable\n",
        "        Should accept variable number of objects of type\n",
        "        `torch.autograd.Variable` to compute its output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, expression_fn):\n",
        "        super(Expression, self).__init__()\n",
        "        self.expression_fn = expression_fn\n",
        "\n",
        "    def forward(self, *x):\n",
        "        return self.expression_fn(*x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        if hasattr(self.expression_fn, \"func\") and hasattr(\n",
        "            self.expression_fn, \"kwargs\"\n",
        "        ):\n",
        "            expression_str = \"{:s} {:s}\".format(\n",
        "                self.expression_fn.func.__name__, str(self.expression_fn.kwargs)\n",
        "            )\n",
        "        elif hasattr(self.expression_fn, \"__name__\"):\n",
        "            expression_str = self.expression_fn.__name__\n",
        "        else:\n",
        "            expression_str = repr(self.expression_fn)\n",
        "        return (\n",
        "            self.__class__.__name__ +\n",
        "            \"(expression=%s) \" % expression_str\n",
        "        )\n",
        "\n",
        "############## Multi-head Attention Mechanism #################################################\n",
        "\n",
        "def scaled_dot_product(q, k, v, mask=None):\n",
        "    d_k = q.size()[-1]\n",
        "    attn_logits = torch.matmul(q, k.transpose(-2, -1))\n",
        "    attn_logits = attn_logits / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15)\n",
        "    attention = F.softmax(attn_logits, dim=-1)\n",
        "    values = torch.matmul(attention, v)\n",
        "    return values, attention\n",
        "\n",
        "class MultiheadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Stack all weight matrices 1...h together for efficiency\n",
        "        # Note that in many implementations you see \"bias=False\" which is optional\n",
        "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
        "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization, see PyTorch documentation\n",
        "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
        "        self.qkv_proj.bias.data.fill_(0)\n",
        "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
        "        self.o_proj.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None, return_attention=False):\n",
        "        batch_size, seq_length, embed_dim = x.size()\n",
        "        qkv = self.qkv_proj(x)\n",
        "\n",
        "        # Separate Q, K, V from linear output\n",
        "        qkv = qkv.reshape(batch_size, seq_length, self.num_heads, 3*self.head_dim)\n",
        "        qkv = qkv.permute(0, 2, 1, 3) # [Batch, Head, SeqLen, Dims]\n",
        "        q, k, v = qkv.chunk(3, dim=-1)\n",
        "\n",
        "        # Determine value outputs\n",
        "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
        "        values = values.permute(0, 2, 1, 3) # [Batch, SeqLen, Head, Dims]\n",
        "        values = values.reshape(batch_size, seq_length, self.embed_dim)\n",
        "        o = self.o_proj(values)\n",
        "\n",
        "        if return_attention:\n",
        "            return o, attention\n",
        "        else:\n",
        "            return o\n",
        "\n",
        "\n",
        "\n",
        "############## stem_stage (reducing temporal dimension) #################################################\n",
        "class stem_stage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k ):\n",
        "        super().__init__()\n",
        "        def _permute(x):\n",
        "            '''\n",
        "            Permutes data:\n",
        "            from dim:\n",
        "            batch, chans, time, 1\n",
        "            to dim:\n",
        "            batch, chans, 1, time'''\n",
        "            return x.permute([0, 1, 3, 2])\n",
        "\n",
        "        prnt=PrintLayer()\n",
        "      \n",
        "        layers = [Ensure4d(),\n",
        "                Expression(_permute),\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=(1,k)),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.GELU()\n",
        "                ]\n",
        "\n",
        "        self.model= nn.Sequential(*layers)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.model(x)\n",
        "\n",
        "############## conv_stage #################################################\n",
        "class conv_stage(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_ch, conv_kernel, pool_kernel):\n",
        "        super().__init__()\n",
        "\n",
        "        prnt=PrintLayer()\n",
        "        self.pool=nn.AvgPool2d(kernel_size=(1, pool_kernel))\n",
        "        self.res=nn.Conv2d(in_channels, out_channels, kernel_size=(1,1), padding='same')\n",
        "        layers = [nn.BatchNorm2d(in_channels),\n",
        "                  nn.Conv2d(in_channels, out_channels, kernel_size=(1,1), padding='same'),\n",
        "                  nn.Conv2d(in_channels=n_ch, out_channels=n_ch, kernel_size=(1,conv_kernel),groups=n_ch, padding='same'),\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=(1,1), padding='same'),\n",
        "                nn.ReLU()\n",
        "                ]\n",
        "\n",
        "        self.model= nn.Sequential(*layers)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.pool(self.res(x))+self.pool(self.model(x))\n",
        "\n",
        "############## InceptionBlock #################################################\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, n_ch, conv_kernel, pool_kernel):        \n",
        "        super().__init__()\n",
        "\n",
        "        self.branch1 = conv_stage(in_channels, out_channels, n_ch, conv_kernel=100, pool_kernel=2)\n",
        "        self.branch2 = conv_stage(in_channels, out_channels, n_ch, conv_kernel=5, pool_kernel=2)\n",
        "        self.branch3 = conv_stage(in_channels, out_channels, n_ch, conv_kernel=3, pool_kernel=2)\n",
        "        self.branch4 = conv_stage(in_channels, out_channels, n_ch, conv_kernel=2, pool_kernel=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branches = (self.branch1, self.branch2, self.branch3, self.branch4)\n",
        "        return torch.cat([branch(x) for branch in branches], 1)\n",
        "\n",
        "\n",
        "############## att_stage #################################################\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x): \n",
        "        return self.net(x)\n",
        "        \n",
        "class att_stage(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, in_channels, temp_dim, pool_kernel, incep=1, masking_rate=0.4):\n",
        "        super().__init__()\n",
        "\n",
        "        prnt=PrintLayer()\n",
        "        self.mr=masking_rate\n",
        "        self.incep=incep\n",
        "        self.ed=embed_dim\n",
        "        self.ic= in_channels\n",
        "        self.ln=nn.LayerNorm(temp_dim)\n",
        "        self.mha=MultiheadAttention(temp_dim ,embed_dim, num_heads)\n",
        "        self.ffn=FeedForward(temp_dim, temp_dim, embed_dim)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x=torch.squeeze(x)\n",
        "      #x1=self.pool(x)\n",
        "      x1=self.ffn(x)\n",
        "      x=self.ln(x)\n",
        "      #x=self.pool(x)\n",
        "      mask=(torch.cuda.FloatTensor(self.ic*self.incep, self.ic*self.incep).uniform_() > self.mr).type(torch.uint8)#.to('cpu')\n",
        "      x,s=self.mha(x, mask, return_attention=True)\n",
        "      return x+x1.reshape([-1,self.ic*self.incep,self.ed]),  s\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#residual block\n",
        "class residual_block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, k, p,stride=1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          in_channels (int):  Number of input channels.\n",
        "          out_channels (int): Number of output channels.\n",
        "          stride (int):       Controls the stride.\n",
        "        \"\"\"\n",
        "        super(residual_block, self).__init__()\n",
        "\n",
        "        self.skip = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "          self.skip = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=1, padding='same', bias=False),\n",
        "            nn.AvgPool2d(kernel_size=(1,p)),\n",
        "            nn.BatchNorm2d(out_channels))\n",
        "        else:\n",
        "          self.skip = None\n",
        "\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1,k), padding='same', bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "            nn.AvgPool2d(kernel_size=(1,p)),\n",
        "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=(1,k), padding='same', bias=False),\n",
        "            nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "      out = self.block(x)\n",
        "      out += (x if self.skip is None else self.skip(x))\n",
        "      out = F.relu(out)\n",
        "      return out\n",
        "\n",
        "\n",
        "############## CoatNet #################################################\n",
        "class coatnet(nn.Module):\n",
        "    def __init__(self, n_classes, n_channels, embed_dim, num_heads, conv_kernel, pool_kernel, incep=4):\n",
        "        super().__init__()\n",
        "\n",
        "        n_ch=n_channels\n",
        "        self.s0=stem_stage(n_ch, n_ch, pool_kernel)\n",
        "        self.res1=residual_block(n_ch, n_ch, 64, 2,stride=2)\n",
        "        self.res2=residual_block(n_ch, n_ch, 32, 2,stride=2)\n",
        "        self.res3=residual_block(n_ch, n_ch, 16, 2,stride=2)\n",
        "        self.s11= InceptionBlock(n_ch, n_ch, n_ch, conv_kernel, pool_kernel)\n",
        "        temp_dim=70*2\n",
        "        self.s2=att_stage(embed_dim, num_heads, n_ch, temp_dim, pool_kernel, incep)\n",
        "        temp_dim=256\n",
        "        self.s22=att_stage(embed_dim, num_heads, n_ch, temp_dim, pool_kernel, incep)\n",
        "\n",
        "        self.classifier=nn.Linear(5632, n_classes)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x=self.s0(x)\n",
        "      x=self.res1(x)\n",
        "      x=self.res2(x)\n",
        "      x=self.res3(x)\n",
        "      #x=self.s11(x)\n",
        "      x,s= self.s2(x)\n",
        "      #x,s= self.s22(x)\n",
        "\n",
        "      x=torch.flatten(x, 1)\n",
        "      x=self.classifier(x)\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRVTk7t38lYW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c8c35c9-960e-4c1d-9a54-8fae8ad2a3ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([60, 22, 1, 140])\n",
            "torch.Size([60, 22, 256])\n"
          ]
        }
      ],
      "source": [
        "eeg = torch.randn(60,22,1125)\n",
        "network=coatnet(n_classes=4, n_channels=22, embed_dim=256, num_heads=4, conv_kernel=10, pool_kernel=4, incep=1)\n",
        "out=network(eeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational Auto-Encoder test"
      ],
      "metadata": {
        "id": "SYGu8lghuney"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VariationalEncoder(nn.Module):\n",
        "    def __init__(self, n_ch, latent_dims):    \n",
        "        super(VariationalEncoder, self).__init__()\n",
        "\n",
        "        # Layer 1\n",
        "        self.conv1 = nn.Conv2d(1, 16, (1, 25), padding = 0)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(16, False)\n",
        "        \n",
        "        # Layer 2\n",
        "        self.padding1 = nn.ZeroPad2d((16, 17, 0, 1))\n",
        "        self.conv2 = nn.Conv2d(1, 4, (2, 13))\n",
        "        self.batchnorm2 = nn.BatchNorm2d(4, False)\n",
        "        self.pooling2 = nn.MaxPool2d(2, 4)\n",
        "        \n",
        "        # Layer 3\n",
        "        self.padding2 = nn.ZeroPad2d((2, 1, 4, 3))\n",
        "        self.conv3 = nn.Conv2d(4, 4, (8, 4))\n",
        "        self.batchnorm3 = nn.BatchNorm2d(4, False)\n",
        "        self.pooling3 = nn.MaxPool2d((2, 4))\n",
        "      \n",
        "\n",
        "        self.linear1 = nn.Linear(568, 128)\n",
        "        self.linear2 = nn.Linear(128, latent_dims)\n",
        "        self.linear3 = nn.Linear(128, latent_dims)\n",
        "\n",
        "        self.N = torch.distributions.Normal(0, 1)\n",
        "        self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
        "        self.N.scale = self.N.scale.cuda()\n",
        "        self.kl = 0\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x=x.reshape([-1,1,1125,25])\n",
        "        x = F.elu(self.conv1(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        \n",
        "        # Layer 2\n",
        "        x = self.padding1(x)\n",
        "        x = F.elu(self.conv2(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling2(x)\n",
        "        \n",
        "        # Layer 3\n",
        "        x = self.padding2(x)\n",
        "        x = F.elu(self.conv3(x))\n",
        "        x = self.batchnorm3(x)\n",
        "        x = F.dropout(x, 0.25)\n",
        "        x = self.pooling3(x)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        mu =  self.linear2(x)\n",
        "        sigma = torch.exp(self.linear3(x))\n",
        "        z = mu + sigma*self.N.sample(mu.shape)\n",
        "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum()\n",
        "        return z      \n",
        "\n",
        "class Decoder(nn.Module): \n",
        "    def __init__(self, n_ch, latent_dims):\n",
        "        super().__init__()\n",
        "\n",
        "        self.decoder_lin = nn.Sequential(\n",
        "            nn.Linear(latent_dims, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 568),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(1, 568))\n",
        "\n",
        "        self.decoder_conv = nn.Sequential(\n",
        "            nn.ConvTranspose1d(1, 16, 4, stride=2, output_padding=0),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(16, 8, 3, stride=1, padding=1, output_padding=1),\n",
        "            nn.BatchNorm1d(8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose1d(8, n_ch, 4, stride=1, padding=1, output_padding=1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.decoder_lin(x)\n",
        "        x = self.unflatten(x)\n",
        "        x = self.decoder_conv(x)\n",
        "        print(x.shape)\n",
        "        return x\n",
        "        \n",
        "class VariationalAutoencoder(nn.Module):\n",
        "    def __init__(self, n_ch, latent_dims):\n",
        "        super(VariationalAutoencoder, self).__init__()\n",
        "        self.encoder = VariationalEncoder(n_ch, latent_dims)\n",
        "        self.decoder = Decoder(n_ch, latent_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(x)\n",
        "        return self.decoder(z)\n",
        "      "
      ],
      "metadata": {
        "id": "IzCi5VnSVJem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg = torch.randn(10,25,1125).to('cuda')\n",
        "model=VariationalAutoencoder(25,1024).to('cuda')\n",
        "out=model(eeg)"
      ],
      "metadata": {
        "id": "WkE55CTsuxOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "\n",
        "vae = VariationalAutoencoder(n_ch=25, latent_dims=1024).to(device)\n",
        "lr = 1e-3\n",
        "optim_vae = torch.optim.Adam(vae.parameters(), lr=lr, weight_decay=1e-5)\n",
        "\n",
        "def train_vae(vae, optimizer, T_x, T_y, batch_size):\n",
        "    # Set train mode for both the encoder and the decoder\n",
        "    vae.train()\n",
        "    train_loss = 0.0\n",
        "    data_perm = torch.randperm(T_x.shape[0])\n",
        "\n",
        "    for i in range(T_x.shape[0] // batch_size):# + (1 if T_x.shape[0] % batch_size != 0 else 0)):\n",
        "        data, target = T_x[data_perm[batch_size*i: batch_size*(i+1)]], T_y[data_perm[batch_size*i: batch_size*(i+1)]]        \n",
        "\n",
        "\n",
        "        x=data\n",
        "\n",
        "        # Forward pass \n",
        "        x_hat = vae(x)\n",
        "        # Evaluate loss\n",
        "        loss = ((x - x_hat)**2).sum() + vae.encoder.kl\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # Print batch loss\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "    return train_loss / T_x.shape[0]\n",
        "\n",
        "def test_vae(vae,T_x, T_y, batch_size):\n",
        "    # Set evaluation mode for encoder and decoder\n",
        "    vae.eval()\n",
        "    val_loss = 0.0\n",
        "    data_perm = torch.randperm(T_x.shape[0])\n",
        "    with torch.no_grad(): # No need to track the gradients\n",
        "        for i in range(T_x.shape[0] // batch_size):# + (1 if T_x.shape[0] % batch_size != 0 else 0)):\n",
        "            data, target = T_x[data_perm[batch_size*i: batch_size*(i+1)]], T_y[data_perm[batch_size*i: batch_size*(i+1)]]  \n",
        "            x=data\n",
        "            # Encode data\n",
        "            encoded_data = vae.encoder(x)\n",
        "            # Decode data\n",
        "            x_hat = vae(x)\n",
        "            loss = ((x - x_hat)**2).sum() + vae.encoder.kl\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    return val_loss / T_x.shape[0]\n",
        "\n",
        "def vae_training(vae, optimizer, T_x, T_y, V_x, V_y,batch_size):\n",
        "    num_epochs = 2\n",
        "    for epoch in range(num_epochs):\n",
        "      train_loss = train_vae(vae, optimizer, T_x, T_y, batch_size)\n",
        "      val_loss = test_vae(vae,V_x, V_y, batch_size)\n",
        "      print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
        "\n",
        "    #plot latent space      \n",
        "    encoded_samples = []\n",
        "    for i in range(V_x.shape[0]):\n",
        "        img = V_x[i].unsqueeze(0)\n",
        "        label = V_y[i]\n",
        "        # Encode image\n",
        "        vae.eval()\n",
        "        with torch.no_grad():\n",
        "            encoded_img  = vae.encoder(img)\n",
        "        # Append to list\n",
        "        encoded_img = encoded_img.flatten().cpu().numpy()\n",
        "        encoded_sample = {f\"Enc. Variable {i}\": enc for i, enc in enumerate(encoded_img)}\n",
        "        encoded_sample['label'] = label\n",
        "        encoded_samples.append(encoded_sample)\n",
        "        \n",
        "    encoded_samples = pd.DataFrame(encoded_samples)\n",
        "\n",
        "    tsne = TSNE(n_components=2)\n",
        "    tsne_results = tsne.fit_transform(encoded_samples.drop(['label'],axis=1))\n",
        "\n",
        "    fig = px.scatter(tsne_results, x=0, y=1, color=encoded_samples.label.astype(str),labels={'0': 'tsne-2d-one', '1': 'tsne-2d-two'})\n",
        "    fig.show()   \n",
        "\n",
        "    data1=V_x[0]\n",
        "    print(data1.shape)\n",
        "        \n",
        "    sampling_rate = 128\n",
        "    info = mne.create_info(25, sampling_rate)\n",
        "    data2  = vae(data1.reshape([1,25,1125]))\n",
        "    raw1=mne.io.RawArray(data1.reshape([25,1125]).detach().cpu().numpy(), info)\n",
        "    raw2=mne.io.RawArray(data2.reshape([25,1125]).detach().cpu().numpy(), info)\n",
        "    print('plot 1')\n",
        "    raw1.plot()\n",
        "    print('plot 2')\n",
        "    raw2.plot()"
      ],
      "metadata": {
        "id": "LDa61I0obe3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c5rEW-BJia4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick only-MLP test"
      ],
      "metadata": {
        "id": "iirnKmTTkpoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############## MLP blocks #################################################\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, arch, vae):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        self.va=vae.encoder\n",
        "        #self.p1 = nn.AvgPool2d(kernel_size=(1, 20))\n",
        "        self.p2 = nn.MaxPool2d(kernel_size=(1, 2))\n",
        "        prnt=PrintLayer()\n",
        "        in_channels=1024\n",
        "        for x in arch:\n",
        "          layers += [nn.Linear(in_channels, x),\n",
        "                      nn.BatchNorm1d(x),\n",
        "                      nn.ReLU() ]\n",
        "          in_channels = x\n",
        "\n",
        "        self.model= nn.Sequential(*layers)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      #x=x.reshape([-1,64,1,480])\n",
        "      #x = torch.flatten(x, 1)\n",
        "      x=self.va(x)\n",
        "      return self.model(x)\n"
      ],
      "metadata": {
        "id": "wRqZNohuk0ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        #self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        # -> x needs to be: (batch_size, seq, input_size)\n",
        "        \n",
        "        # or:\n",
        "        #self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Set initial hidden states (and cell states for LSTM)\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
        "        \n",
        "        # x: (n, 28, 28), h0: (2, n, 128)\n",
        "        \n",
        "        # Forward propagate RNN\n",
        "        #out, _ = self.rnn(x, h0)  \n",
        "        # or:\n",
        "        out, _ = self.lstm(x, (h0,c0))  \n",
        "        \n",
        "        # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "        # out: (n, 28, 128)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = out[:, -1, :]\n",
        "        # out: (n, 128)\n",
        "         \n",
        "        out = self.fc(out)\n",
        "        # out: (n, 10)\n",
        "        return out"
      ],
      "metadata": {
        "id": "nk2r2vX3e8wk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg = torch.randn(10,25,1125).to('cuda')\n",
        "model=RNN(input_size=1125, hidden_size=128 , num_layers=4, num_classes=4).to('cuda')\n",
        "out=model(eeg)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "Dl0dA2gXmvpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg = torch.randn(64,480)\n",
        "arch=[512,4]\n",
        "#model=MLP(arch)\n",
        "#out=model(eeg)"
      ],
      "metadata": {
        "id": "qh6Eio-wp79C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHgle-E9cYTo"
      },
      "source": [
        "###Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "id": "oWBlu9QAcYEw",
        "outputId": "26adf323-0e71-4d40-fe3b-d2132f05fbba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    def forward(self, x):\\n        \"\"\"Forward pass.\\n        Parameters\\n        ----------\\n        x: torch.Tensor\\n            Batch of EEG windows of shape (batch_size, n_channels, n_times).\\n        \"\"\"\\n        if self.mode!=\\'1x1\\':\\n            x = self.tidnet_temp(x)\\n        \\n        if self.mode!=\\'mlp\\':\\n            x = self.model(x)\\n            x = torch.flatten(x, 1)\\n            out = self.fc_block(x)\\n\\n        else :\\n            x = torch.flatten(x, 1)\\n            x = self.model(x)\\n            out = self.fc_block(x)\\n        return out\\n    \\n    def get_emb(self, x):\\n        return self.tidnet_temp(x)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from math import ceil\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils import weight_norm\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "##############  TIDNet modules #############################################################\n",
        "\n",
        "class PrintLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintLayer, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Do your print / debug stuff here\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "class Ensure4d(nn.Module):\n",
        "    def forward(self, x):\n",
        "        while(len(x.shape) < 4):\n",
        "            x = x.unsqueeze(-1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Expression(nn.Module):\n",
        "    \"\"\"Compute given expression on forward pass.\n",
        "    Parameters\n",
        "    ----------\n",
        "    expression_fn : callable\n",
        "        Should accept variable number of objects of type\n",
        "        `torch.autograd.Variable` to compute its output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, expression_fn):\n",
        "        super(Expression, self).__init__()\n",
        "        self.expression_fn = expression_fn\n",
        "\n",
        "    def forward(self, *x):\n",
        "        return self.expression_fn(*x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        if hasattr(self.expression_fn, \"func\") and hasattr(\n",
        "            self.expression_fn, \"kwargs\"\n",
        "        ):\n",
        "            expression_str = \"{:s} {:s}\".format(\n",
        "                self.expression_fn.func.__name__, str(self.expression_fn.kwargs)\n",
        "            )\n",
        "        elif hasattr(self.expression_fn, \"__name__\"):\n",
        "            expression_str = self.expression_fn.__name__\n",
        "        else:\n",
        "            expression_str = repr(self.expression_fn)\n",
        "        return (\n",
        "            self.__class__.__name__ +\n",
        "            \"(expression=%s) \" % expression_str\n",
        "        )\n",
        "\n",
        "\n",
        "class _TemporalFilter(nn.Module):\n",
        "    def __init__(self, in_chans, filters, depth, temp_len, drop_prob=0., activation=nn.LeakyReLU,\n",
        "                 residual='netwise'):\n",
        "        super().__init__()\n",
        "        temp_len = temp_len + 1 - temp_len % 2\n",
        "        self.residual_style = str(residual)\n",
        "        net = list()\n",
        "\n",
        "        for i in range(depth):\n",
        "            dil = depth - i\n",
        "            conv = weight_norm(nn.Conv2d(in_chans if i == 0 else filters, filters,\n",
        "                                         kernel_size=(1, temp_len), dilation=dil,\n",
        "                                         padding=(0, dil * (temp_len - 1) // 2)))\n",
        "            net.append(nn.Sequential(\n",
        "                conv,\n",
        "                activation(),\n",
        "                nn.Dropout2d(drop_prob)\n",
        "            ))\n",
        "        if self.residual_style.lower() == 'netwise':\n",
        "            self.net = nn.Sequential(*net)\n",
        "            self.residual = nn.Conv2d(in_chans, filters, (1, 1))\n",
        "        elif residual.lower() == 'dense':\n",
        "            self.net = net\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual_style.lower() == 'netwise':\n",
        "            return self.net(x) + self.residual(x)\n",
        "        elif self.residual_style.lower() == 'dense':\n",
        "            for layer in self.net:\n",
        "                x = torch.cat((x, layer(x)), dim=1)\n",
        "            return x\n",
        "\n",
        "\n",
        "############## MLP blocks #################################################\n",
        "class MLP_blocks(nn.Module):\n",
        "    def __init__(self, arch, in_channels):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        prnt=PrintLayer()\n",
        "        for x in arch:\n",
        "            layers += [nn.Linear(in_channels, x),\n",
        "                        nn.BatchNorm1d(x),\n",
        "                        nn.ReLU()]\n",
        "            in_channels = x\n",
        "\n",
        "        self.model= nn.Sequential(*layers)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.model(x)\n",
        "\n",
        "############## spatial convs(electrodes,1) (electrode aggregator)  (still on testing phase) #################################################\n",
        "class spatial_aggregator(nn.Module):\n",
        "    def __init__(self, arch, in_channels, k ):\n",
        "        super().__init__()\n",
        "        def _permute(x):\n",
        "            \"\"\"\n",
        "            Permutes data:\n",
        "            from dim:\n",
        "            batch, chans, time, 1\n",
        "            to dim:\n",
        "            batch, 1, chans, time\n",
        "            \"\"\"\n",
        "            #return x.permute([0, 3, 1, 2])\n",
        "            return x.permute([0, 1, 3, 2])\n",
        "\n",
        "        prnt=PrintLayer()\n",
        "      \n",
        "        if in_channels==1:\n",
        "            layers = [Ensure4d(),\n",
        "                Expression(_permute)]\n",
        "        else:\n",
        "            layers = []\n",
        "        #prnt=PrintLayer()\n",
        "            \n",
        "        for x in arch:\n",
        "            s=str(x)\n",
        "            if s[0] == 'A':\n",
        "                layers += [nn.AvgPool2d(kernel_size=(1, int(s[1:])), stride=(1,2))]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels=64, out_channels=64, kernel_size=(1,10),groups=64),\n",
        "                           #nn.Conv2d(in_channels, x, kernel_size=(1,k)), \n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU()]\n",
        "                in_channels = x\n",
        "\n",
        "        self.model= nn.Sequential(*layers)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.model(x)\n",
        "\n",
        "############## electrode weight-sharing convs(1,1) (a.k.a 1x1encoders) #################################################\n",
        "class SharedSpaceTimeConv1x1(nn.Module):\n",
        "    def __init__(self, mode, arch, in_channels):\n",
        "        super().__init__()\n",
        "        def _permute(x):\n",
        "            \"\"\"\n",
        "            Permutes data:\n",
        "            from dim:\n",
        "            batch, chans, time, 1\n",
        "            to dim:\n",
        "            batch, 1, chans, time\n",
        "            \"\"\"\n",
        "            return x.permute([0, 3, 1, 2])\n",
        "        prnt=PrintLayer()\n",
        "        if mode=='1x1':\n",
        "            layers = [Ensure4d(),\n",
        "                Expression(_permute),]\n",
        "            in_channels = 1\n",
        "        else:\n",
        "            layers = []\n",
        "        for x in arch:\n",
        "            s=str(x)\n",
        "            if s[0] == 'A':\n",
        "                layers += [nn.AvgPool2d(kernel_size=(1, int(s[1:])), stride=(1,2))]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU()]\n",
        "                in_channels = x\n",
        "\n",
        "        self.model= nn.Sequential(*layers)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      return self.model(x)\n",
        "\n",
        "\n",
        "############## TIDNet temporal block #################################################\n",
        "class TemporalTIDNet(nn.Module):\n",
        "    def __init__(self, t_filters, input_window_samples, drop_prob, pooling,\n",
        "                 temp_layers,  temp_span):\n",
        "        super().__init__()\n",
        "        self.temp_len = ceil(temp_span * input_window_samples)\n",
        "\n",
        "        def _permute(x):\n",
        "            \"\"\"\n",
        "            Permutes data:\n",
        "            from dim:\n",
        "            batch, chans, time, 1\n",
        "            to dim:\n",
        "            batch, 1, chans, time\n",
        "            \"\"\"\n",
        "            return x.permute([0, 3, 1, 2])\n",
        "\n",
        "        self.temporal = nn.Sequential(\n",
        "            Ensure4d(),\n",
        "            Expression(_permute),\n",
        "            _TemporalFilter(1, t_filters, depth=temp_layers, temp_len=self.temp_len),\n",
        "            nn.MaxPool2d((1, pooling)),\n",
        "            nn.Dropout2d(drop_prob),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.temporal(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "###################################################################################### \n",
        "\n",
        "class TemporalShareSpaceTime(nn.Module):\n",
        "\n",
        "    def __init__(self, mode, arch, n_classes, in_chans, input_window_samples, t_filters,\n",
        "                 drop_prob, pooling, temp_layers, temp_span):\n",
        "        super().__init__()\n",
        "        def compute_params(arch,h,w):\n",
        "            ### height and width of the input convolutions of each feature map\n",
        "            #### compute the number of inputs after flatten ######\n",
        "            for i in arch:\n",
        "                s=str(i)\n",
        "                if s[0] == 'A':\n",
        "                    w=int((w-int(s[1:]))/2 +1 )     #for average pooling we apply:  [(Output width + padding width right + padding width left - kernel width) / (stride width)] + 1\n",
        "            s=str(i)\n",
        "            if s[0] == 'A':\n",
        "                last_filter=arch[-2]\n",
        "            else:\n",
        "                last_filter=arch[-1]\n",
        "            return int(last_filter*h*w)\n",
        "\n",
        "        self.mode=mode\n",
        "        self.n_classes = n_classes\n",
        "        self.in_chans = in_chans\n",
        "        self.input_window_samples = input_window_samples\n",
        "        self.temp_len = ceil(temp_span * input_window_samples)\n",
        "        self.params=compute_params(arch,in_chans,ceil((input_window_samples/pooling)-1))  \n",
        "\n",
        "        #self.TrnsEnc= TransformerEncoder(num_layers=8 ,\n",
        "                      #                        input_dim=input_window_samples,\n",
        "                      #                        dim_feedforward=56,\n",
        "                       #                       num_heads=8,\n",
        "                         #                     dropout=0.2)# CoAtNet(2,480)\n",
        "\n",
        "\n",
        "        self.tidnet_temp = TemporalTIDNet(t_filters=t_filters,\n",
        "                                     input_window_samples=input_window_samples,\n",
        "                                     drop_prob=drop_prob, pooling=pooling, temp_layers=temp_layers,\n",
        "                                     temp_span=temp_span)\n",
        "        \n",
        "        \n",
        "        self.MHAtn=MultiheadAttention(input_dim=input_window_samples, embed_dim=512, num_heads=8)\n",
        "        self.MHAtn2=MultiheadAttention(input_dim=64, embed_dim=128, num_heads=4)\n",
        "\n",
        "        self.MHAtn2=nn.MultiheadAttention(embed_dim=107, num_heads=1)\n",
        "\n",
        "        mode=[64,'A10',64,'A10']\n",
        "        self.rd= spatial_aggregator(mode, 1, 10)\n",
        "        #self.rd2= spatial_aggregator([16,1], 1, 5)\n",
        "        \n",
        "\n",
        "\n",
        "        \n",
        "        ######################################################\n",
        "        \n",
        "        if self.mode!='1x1':\n",
        "            self.tidnet_temp = TemporalTIDNet(t_filters=t_filters,\n",
        "                                     input_window_samples=input_window_samples,\n",
        "                                     drop_prob=drop_prob, pooling=pooling, temp_layers=temp_layers,\n",
        "                                     temp_span=temp_span)     \n",
        "\n",
        "        if self.mode=='1x1' or self.mode=='t+1x1':\n",
        "            self.model = SharedSpaceTimeConv1x1(self.mode, arch, t_filters) \n",
        "            if self.mode=='1x1':\n",
        "                self.params=compute_params(arch,in_chans,input_window_samples)\n",
        "            self.fc_block = nn.Linear(self.params, self.n_classes)\n",
        "        elif self.mode=='mlp':\n",
        "            self.model = MLP_blocks(arch, in_chans*ceil((input_window_samples/pooling))*t_filters)\n",
        "            self.fc_block = nn.Linear(arch[-1], self.n_classes)\n",
        "\n",
        "        self.fc_block2 = nn.Linear(3200, self.n_classes)\n",
        "\n",
        "        #for transformer test\n",
        "    def forward(self, x):\n",
        "\n",
        "      x=self.rd(x)\n",
        "      print(x.shape)\n",
        "      x=torch.squeeze(x)\n",
        "      x,_ = self.MHAtn2(x,x,x)\n",
        "      #print(x.shape)\n",
        "      #x=x.reshape([-1,25,238])\n",
        "      #x= self.MHAtn2(x)\n",
        "      #x=self.rd2(x)\n",
        "      x = torch.flatten(x, 1)\n",
        "      x = self.fc_block2(x)\n",
        "      return x\n",
        "    \n",
        "\n",
        "    def get_emb(self, x):\n",
        "        return self.tidnet_temp(x)\n",
        "\n",
        "    \n",
        "'''\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
        "        \"\"\"\n",
        "        if self.mode!='1x1':\n",
        "            x = self.tidnet_temp(x)\n",
        "        \n",
        "        if self.mode!='mlp':\n",
        "            x = self.model(x)\n",
        "            x = torch.flatten(x, 1)\n",
        "            out = self.fc_block(x)\n",
        "\n",
        "        else :\n",
        "            x = torch.flatten(x, 1)\n",
        "            x = self.model(x)\n",
        "            out = self.fc_block(x)\n",
        "        return out\n",
        "    \n",
        "    def get_emb(self, x):\n",
        "        return self.tidnet_temp(x)'''\n",
        "\n",
        "############################################################################\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFbiJbUcchBA"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LppaUdf-chNA"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import os\n",
        "import torch \n",
        "from torch import optim\n",
        "from sklearn.metrics import f1_score\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchinfo import summary\n",
        "import gc\n",
        "\n",
        "\n",
        "\n",
        "from braindecode.models import EEGNetv4,TIDNet, EEGResNet\n",
        "def build_network(mode, arch, n_classes=4, in_chans=25, input_window_samples=1125, t_filters=32,\n",
        "                 drop_prob=0.4, pooling=15, temp_layers=2, temp_span=0.05):\n",
        "    #model=coatnet(n_classes=4, n_channels=22, embed_dim=256, num_heads=4, conv_kernel=10, pool_kernel=4, incep=1)\n",
        "    #model=RNN(input_size=1125, hidden_size=128 , num_layers=4, num_classes=4).to('cuda')\n",
        "    #model=TemporalShareSpaceTime(mode, arch, n_classes, in_chans, input_window_samples, t_filters,\n",
        "     #            drop_prob, pooling, temp_layers, temp_span)\n",
        "    #model=MLP(arch, vae)\n",
        "    model=EEGNetv4(in_chans, n_classes, input_window_samples)\n",
        "  \n",
        "    return model\n",
        "\n",
        "\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9, weight_decay=0.5*0.001)\n",
        "    elif optimizer == \"adamw\":\n",
        "        optimizer = optim.AdamW(network.parameters(),\n",
        "                               lr=learning_rate,  weight_decay=0.01, amsgrad=True)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "\n",
        "####### Training using mixup #####################################################################\n",
        "def mixup_data(x, y, alpha=8.0, use_cuda=True):\n",
        "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
        "    if alpha > 0:\n",
        "        lam = np.random.beta(alpha, alpha)\n",
        "    else:\n",
        "        lam = 1\n",
        "\n",
        "    batch_size = x.size()[0]\n",
        "    if use_cuda:\n",
        "        index = torch.randperm(batch_size).cuda()\n",
        "    else:\n",
        "        index = torch.randperm(batch_size)\n",
        "\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, y_a, y_b, lam\n",
        "\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
        "\n",
        "\n",
        "def train_mixup(network, optimizer, T_x, T_y, batch_size, tr_iter):\n",
        "    cumu_loss = 0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    data_perm = torch.randperm(T_x.shape[0])\n",
        "    criterion = torch.nn.CrossEntropyLoss()   \n",
        "    \n",
        "    network.train()\n",
        "\n",
        "    for i in range(T_x.shape[0] // batch_size + (1 if T_x.shape[0] % batch_size != 0 else 0)):\n",
        "        data, target = T_x[data_perm[batch_size*i: batch_size*(i+1)]], T_y[data_perm[batch_size*i: batch_size*(i+1)]] \n",
        "\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass :\n",
        "        ## implement mixup with alpha preset to 2\n",
        "        inputs, targets_a, targets_b, lam = mixup_data(data, target, use_cuda=True)\n",
        "        inputs, targets_a, targets_b = map(Variable, (inputs,\n",
        "                                                    targets_a, targets_b))\n",
        "\n",
        "\n",
        "        outputs = network(inputs)\n",
        "\n",
        "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
        "        \n",
        "        cumu_loss += loss.item()\n",
        "    \n",
        "        # ⬅ Backward pass + weight update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # compute accuracy\n",
        "        # Get predictions from the maximum value\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Total number of labels\n",
        "        total += target.size(0)\n",
        "        correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()\n",
        "                    + (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())\n",
        "\n",
        "    \n",
        "        tr_iter=tr_iter+1\n",
        "\n",
        "    return cumu_loss / T_x.shape[0], correct/total, tr_iter\n",
        "############################################################################\n",
        "\n",
        "def train_epoch(network, optimizer, T_x, T_y, batch_size, tr_iter):\n",
        "    cumu_loss = 0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    data_perm = torch.randperm(T_x.shape[0])\n",
        "    \n",
        "    network.train()\n",
        "\n",
        "    for i in range(T_x.shape[0] // batch_size):# + (1 if T_x.shape[0] % batch_size != 0 else 0)):\n",
        "        data, target = T_x[data_perm[batch_size*i: batch_size*(i+1)]], T_y[data_perm[batch_size*i: batch_size*(i+1)]]        \n",
        "\n",
        "        r=random.uniform(0, 1)\n",
        "        #if r<0.4:\n",
        "          #data=apply_da(data,selection)\n",
        "        # Clear the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass \n",
        "\n",
        "        outputs = network(data)\n",
        "        loss = F.cross_entropy(outputs, target)\n",
        "        cumu_loss += loss.item()\n",
        "    \n",
        "        # ⬅ Backward pass + weight update\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "        # compute accuracy\n",
        "        # Get predictions from the maximum value\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Total number of labels\n",
        "        total += target.size(0)\n",
        "        correct += (predicted == target).sum()\n",
        "\n",
        "\n",
        "        tr_iter=tr_iter+1\n",
        "\n",
        "    return cumu_loss / T_x.shape[0], correct/total, tr_iter\n",
        "\n",
        "\n",
        "def validate_epoch(network, T_x, T_y, batch_size, v_itr):\n",
        "    cumu_loss = 0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    data_perm = torch.randperm(T_x.shape[0])\n",
        "    \n",
        "    network.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i in range(T_x.shape[0] // batch_size + (1 if T_x.shape[0] % batch_size != 0 else 0)):\n",
        "            data, target = T_x[data_perm[batch_size*i: batch_size*(i+1)]], T_y[data_perm[batch_size*i: batch_size*(i+1)]]   \n",
        "\n",
        "            loss = F.cross_entropy(network(data), target)\n",
        "            cumu_loss += loss.item()     \n",
        "\n",
        "            # compute accuracy\n",
        "            outputs = network(data)\n",
        "\n",
        "            # Get predictions from the maximum value\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Total number of labels\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum()   \n",
        "\n",
        "            v_itr=v_itr+1\n",
        "    \n",
        "    return cumu_loss / T_x.shape[0], correct/total, v_itr\n",
        "\n",
        "\n",
        "def test(network, T_x, T_y, batch_size, classes, t_itr):\n",
        "    n_classes=len(classes)\n",
        "    # Calculate Accuracy\n",
        "    correct = 0.0\n",
        "    correct_arr = [0.0] * n_classes\n",
        "    total = 0.0\n",
        "    total_arr = [0.0] * n_classes\n",
        "    y_true=[]\n",
        "    y_pred=[]\n",
        "    pred_probs=[]\n",
        "    # Iterate through test dataset\n",
        "    data_perm = torch.randperm(T_x.shape[0])\n",
        "    \n",
        "    network.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i in range(T_x.shape[0] // batch_size + (1 if T_x.shape[0] % batch_size != 0 else 0)):\n",
        "            data, target = T_x[data_perm[batch_size*i: batch_size*(i+1)]], T_y[data_perm[batch_size*i: batch_size*(i+1)]]   \n",
        "\n",
        "            outputs = network(data)\n",
        "            \n",
        "            # Get predictions from the maximum value\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # Total number of labels\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum()\n",
        "            y_true.append(target.cpu().detach().numpy())\n",
        "            y_pred.append(predicted.cpu().detach().numpy())\n",
        "            pred_probs.append(outputs.data.cpu().detach().numpy())\n",
        "            \n",
        "            for label in range(n_classes):\n",
        "                correct_arr[label] += (((predicted == target) & (target==label)).sum())\n",
        "                total_arr[label] += (target == label).sum()\n",
        "    \n",
        "    \n",
        "    y_true=np.array(y_true[:-1]).reshape([-1])\n",
        "    y_pred=np.array(y_pred[:-1]).reshape([-1])\n",
        "    pred_probs=np.array(pred_probs[:-1]).reshape([-1,n_classes])\n",
        "\n",
        "\n",
        "    # Confusion Matrices\n",
        "    wandb.log({\"conf_mat\" : wandb.plot.confusion_matrix(probs=None,\n",
        "                        y_true=y_true , preds=y_pred ,\n",
        "                        class_names=classes)})\n",
        "\n",
        "\n",
        "    # ROC\n",
        "    wandb.log({\"roc\" : wandb.plot.roc_curve(  y_true , pred_probs ,\n",
        "                            labels=classes)})\n",
        "\n",
        "\n",
        "    # Precision Recall Curve\n",
        "    wandb.log({\"pr\" : wandb.plot.pr_curve( y_true , pred_probs ,\n",
        "                        labels=classes, classes_to_plot=None)})\n",
        "\n",
        "    \n",
        "    f1=f1_score( y_true, y_pred, average='macro')\n",
        "    wandb.log({'Test macro F1-Score': f1})\n",
        "    print(f1)\n",
        "    accuracy = correct / total\n",
        "    print('TEST ACCURACY {} '.format(accuracy))\n",
        "            \n",
        "    t_itr=t_itr+1               \n",
        "    return accuracy, f1\n",
        "\n",
        "\n",
        "\n",
        "def train_sweep(x, y, number_of_subjects, device, bad_subjects, config, run, resultdir):\n",
        "    \n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    run_name = run.name\n",
        "    batch_size= 64 #config.batch_size\n",
        "    patience = 20 #config.patience\n",
        "    LR=1e-3 #config.learning_rate\n",
        "    optim='adamw' #config.optimizer\n",
        "    \n",
        "    '''mode=config.mode\n",
        "    design=config.design\n",
        "    p= config.power_2_of_filters\n",
        "    v=1\n",
        "    if mode!='mlp':\n",
        "        v=config.power_2_of_Avg_pooling\n",
        "\n",
        "\n",
        "    \n",
        "    if design=='inverse_bottleneck':\n",
        "        p=p-2\n",
        "        v=1\n",
        "    elif design=='bottleneck':\n",
        "        p=p+3\n",
        "    elif design=='fixed':\n",
        "        v=1\n",
        "    arch=[]\n",
        "\n",
        "    # Building the design\n",
        "    for i in range(config.number_of_layers):\n",
        "        arch.append(2**p)\n",
        "        if mode!='mlp':\n",
        "            arch.append('A'+str(2**v))\n",
        "        if design=='bottleneck':\n",
        "            p-=1\n",
        "            v-=1\n",
        "            if v<1:\n",
        "                v=1\n",
        "        elif design=='inverse_bottleneck':\n",
        "            p+=1\n",
        "            if p>6:\n",
        "                p=6\n",
        "    print(arch)'''\n",
        "\n",
        "\n",
        "    mode='1x1'\n",
        "    arch=[1024,512,128,64,32,4]\n",
        "    network = build_network(mode, arch, n_classes=4, in_chans=22, input_window_samples=1125)\n",
        " \n",
        "    \n",
        "    network.to(device)\n",
        "    pytorch_total_params = sum(p.numel() for p in network.parameters() if p.requires_grad)\n",
        "    wandb.log({\"total number of parameters\": pytorch_total_params})\n",
        "    optimizer = build_optimizer(network, optim, LR)\n",
        "    \n",
        "    classes=['left fist', 'right fist', 'eyes_open', 'MI O/C feet']\n",
        "\n",
        "    kfold= config.subject\n",
        "\n",
        "    T_x,T_y,V_x,V_y,Test_x,Test_y=loso(x, y, number_of_subjects, kfold, device, bad_subjects, with_validation=True)\n",
        "\n",
        "    #vae_training(vae, optim_vae, T_x, T_y, V_x, V_y,batch_size)\n",
        "\n",
        "    Train_acc=[]\n",
        "    Val_acc=[]\n",
        "    Train_loss=[]\n",
        "    Val_loss=[]\n",
        "    Test_acc=[]\n",
        "    ## Tensorboard iterators\n",
        "    tr_iter=0\n",
        "    v_itr=0\n",
        "    maxv=0\n",
        "    cpt_early = 0\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        try :\n",
        "            train_loss,train_acc,tr_iter = train_epoch(network, optimizer, T_x, T_y, batch_size, tr_iter)\n",
        "        except:\n",
        "            #exit()\n",
        "            pass\n",
        "        #print('train loss {} accuracy {} epoch {} done'.format(train_loss,train_acc,epoch))\n",
        "\n",
        "        val_loss,val_acc,v_itr = validate_epoch(network, V_x, V_y, batch_size, v_itr)\n",
        "        #print('val loss {} epoch {} done'.format(val_loss,epoch))\n",
        "\n",
        "        Train_acc.append(train_acc)\n",
        "        Val_acc.append(val_acc)\n",
        "        Train_loss.append(train_loss)\n",
        "        Val_loss.append(val_loss)\n",
        "        wandb.log({'Training accuracy': train_acc,'Training loss': train_loss,'Validation accuracy': val_acc,'Validation loss': val_loss});\n",
        "\n",
        "        \n",
        "        if maxv<val_acc:\n",
        "            print(f\"Epoch {epoch}, new best val accuracy {val_acc} and loss {val_loss}\")\n",
        "            maxv=val_acc\n",
        "\n",
        "            ckpt_dict = {\n",
        "            'weights': network.state_dict(),\n",
        "            'train_acc': Train_acc,\n",
        "            'val_acc': Val_acc,\n",
        "            'train_loss': Train_loss,\n",
        "            'val_acc': Val_loss,\n",
        "            'epoch': epoch\n",
        "            }\n",
        "            torch.save(ckpt_dict,os.path.join(resultdir,f\"{run_name}_bestval.pth\") )\n",
        "            cpt_early = 0\n",
        "        else:\n",
        "            cpt_early +=1\n",
        "        \n",
        "        if cpt_early == patience:\n",
        "            print(\"Early Stopping\")\n",
        "            wandb.log({'Maximum validation accuracy': maxv})\n",
        "            break\n",
        "    \n",
        "    print(\"Reloading best validation model\")\n",
        "    ckpt_dict = torch.load(os.path.join(resultdir,f\"{run_name}_bestval.pth\") )\n",
        "\n",
        "    print(f\"Reloading best model at epoch {ckpt_dict['epoch']}\")\n",
        "    network.load_state_dict(ckpt_dict['weights'])\n",
        "\n",
        "    test_acc,f1=test(network, Test_x, Test_y, batch_size, classes, epoch)\n",
        "    wandb.log({'Test accuracy': test_acc})\n",
        "\n",
        "    return test_acc, f1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drQY84MFcrnA"
      },
      "source": [
        "###Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5g4HZxoccrVx",
        "outputId": "f7dd1fd4-e574-4135-b809-2d41d20f354a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c4d4292bb2ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mnumber_of_subjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mbad_subjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#bad subject for Physionet MI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mload_subjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/bci4_subjects/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_subjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbad_subjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapply_euclidean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_eog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m#number_of_subjects=109\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-557633a81bce>\u001b[0m in \u001b[0;36mload_subjects\u001b[0;34m(path, number_of_subjects, device, bad_subjects, apply_euclidean, with_eog)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnumber_of_subjects\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubject_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbad_subjects\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMOABBDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"BNCI2014001\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/braindecode/datasets/moabb.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_name, subject_ids, dataset_kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mraws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data_with_moabb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         all_base_ds = [BaseDataset(raw, row)\n\u001b[1;32m    110\u001b[0m                        for raw, (_, row) in zip(raws, description.iterrows())]\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/braindecode/datasets/moabb.py\u001b[0m in \u001b[0;36mfetch_data_with_moabb\u001b[0;34m(dataset_name, subject_ids, dataset_kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_find_dataset_in_moabb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0msubject_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msubject_ids\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msubject_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_fetch_and_unpack_moabb_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/braindecode/datasets/moabb.py\u001b[0m in \u001b[0;36m_fetch_and_unpack_moabb_data\u001b[0;34m(dataset, subject_ids)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_fetch_and_unpack_moabb_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mraws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msubj_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubj_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moabb/datasets/base.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, subjects)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubject_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid subject {:d} given\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_single_subject_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moabb/datasets/bnci.py\u001b[0m in \u001b[0;36m_get_single_subject_data\u001b[0;34m(self, subject)\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_single_subject_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;34m\"\"\"return data for a single subject\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0msessions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-702>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(subject, dataset, path, force_update, update_path, base_url, verbose)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moabb/datasets/bnci.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(subject, dataset, path, force_update, update_path, base_url, verbose)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     return dataset_list[dataset](\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseurl_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-703>\u001b[0m in \u001b[0;36m_load_data_001_2014\u001b[0;34m(subject, path, force_update, update_path, base_url, verbose)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moabb/datasets/bnci.py\u001b[0m in \u001b[0;36m_load_data_001_2014\u001b[0;34m(subject, path, force_update, update_path, base_url, verbose)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"T\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"E\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{u}001-2014/A{s:02d}{r}.mat\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mruns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_mi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# FIXME: deal with run with no event (1:3) and name them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moabb/datasets/bnci.py\u001b[0m in \u001b[0;36mdata_path\u001b[0;34m(url, path, force_update, update_path, verbose)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_update\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"BNCI\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-701>\u001b[0m in \u001b[0;36mdata_dl\u001b[0;34m(url, sign, path, force_update, verbose)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/moabb/datasets/download.py\u001b[0m in \u001b[0;36mdata_dl\u001b[0;34m(url, sign, path, force_update, verbose)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mfname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mosp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mprogressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     )\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdlpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pooch/core.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(url, known_hash, fname, path, processor, downloader, progressbar)\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0mdownloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_downloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogressbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mstream_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mknown_hash\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pooch/core.py\u001b[0m in \u001b[0;36mstream_download\u001b[0;34m(url, fname, known_hash, downloader, pooch, retry_if_failed)\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0;31m# hash before overwriting the original.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtemporary_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m                 \u001b[0mdownloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m                 \u001b[0mhash_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknown_hash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                 \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pooch/downloaders.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, url, output_file, pooch)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0moutput_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;31m# Resolve redirects if allowed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;31m# Shuffle things around if there's history.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[0;34m(self, resp, req, stream, timeout, verify, cert, proxies, yield_requests, **adapter_kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m                     \u001b[0;34m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m                 )\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sock'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mca_cert_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mca_cert_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             ssl_context=context)\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_fingerprint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir)\u001b[0m\n\u001b[1;32m    343\u001b[0m             or IS_SECURETRANSPORT):\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         warnings.warn(\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m         )\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m    868\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "project_name=\"sandbox\"\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'grid', #grid\n",
        "    'metric': {\n",
        "      'name': 'loss',\n",
        "      'goal': 'minimize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [3000]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [64]\n",
        "        },\n",
        "        'patience': {\n",
        "            'values': [60]\n",
        "        },\n",
        "     \n",
        "        'learning_rate': {\n",
        "            'values': [1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adamw']\n",
        "        },\n",
        "         'loss': {\n",
        "            'values': ['CrossEntropyLoss'],\n",
        "        },\n",
        "        'subject': {\n",
        "            'values': [1,2,3,4,5,6,7,8,9]\n",
        "        },\n",
        "        'runs': {\n",
        "            'values': [1,2,3]\n",
        "        },\n",
        "      \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
        "\n",
        "number_of_subjects=9\n",
        "bad_subjects=[] #bad subject for Physionet MI\n",
        "x, y= load_subjects('/content/drive/MyDrive/Colab Notebooks/bci4_subjects/', number_of_subjects, device, bad_subjects, apply_euclidean=False, with_eog=False)\n",
        "\n",
        "#number_of_subjects=109\n",
        "#bad_subjects=[88,90,92,100] #bad subject for Physionet MI\n",
        "#x, y= load_subjects('/content/drive/MyDrive/Colab Notebooks/mne_data/', number_of_subjects, device, bad_subjects, apply_euclidean=False)\n",
        "\n",
        "def train_wandb():\n",
        "    # Initialize a new wandb run\n",
        "    run = wandb.init(project=project_name, entity=\"brain-imt\" , config=sweep_config)\n",
        "    assert run is wandb.run\n",
        "    with run:\n",
        "        config = wandb.config\n",
        "        #run=12\n",
        "        test_acc, f1=train_sweep(x, y, number_of_subjects, device, bad_subjects, config, run, resultdir)\n",
        "    ############################################################################\n",
        "\n",
        "#import os\n",
        "#os.environ[\"WANDB_MODE\"]=\"offline\"\n",
        "\n",
        "#Test_acc= train_wandb()\n",
        "wandb.agent(sweep_id, train_wandb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auldJ7s6f3qz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "accs_no_ea=[0.7024, 0.6289, 0.6145, 0.6795, 0.6892, 0.621, 0.6451, 0.6594, 0.6407, 0.5998]\n",
        "accs_ea=[0.7378, 0.7378, 0.8438, 0.6563, 0.6788, 0.599, 0.6059, 0.6233, 0.651]\n",
        "accs_bci=[0.73,0.72,0.84,0.65,0.67,0.599,0.6,0.62,0.65]\n",
        "mlp_bci_accs=[0.57,0.58,0.55,0.6,0.74,0.66,0.81,0.67,0.58]\n",
        "accs=mlp_bci_accs\n",
        "accs=np.array(accs)\n",
        "print('mean ',accs.mean())\n",
        "print('std ',accs.std())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS2mxmM4qx6f"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pceY6sGNaTvE",
        "9HsHADplayNq",
        "SYGu8lghuney",
        "iirnKmTTkpoF",
        "eHgle-E9cYTo"
      ],
      "machine_shape": "hm",
      "name": "sweeps_with_blocks.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMk7wTT/EutQ23fozOJn4Zc",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}