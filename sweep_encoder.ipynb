{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sweep_encoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO/5AA/H6mL2dB8aTklt8S4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GhBlg/Others/blob/main/sweep_encoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moabb\n",
        "!pip install braindecode\n",
        "!pip install wandb\n",
        "!wandb login 33081462b5f17d9fed5c252c3fcb2071d2250425"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cwa7lH50ULDY",
        "outputId": "9c654c7f-ecdf-464a-8ce7-6808cb834ae7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting moabb\n",
            "  Downloading moabb-0.4.4-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.0.2)\n",
            "Requirement already satisfied: seaborn>=0.9 in /usr/local/lib/python3.7/dist-packages (from moabb) (0.11.2)\n",
            "Requirement already satisfied: pooch<2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.6.0)\n",
            "Collecting pyriemann>=0.2.6\n",
            "  Downloading pyriemann-0.2.7.tar.gz (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting mne>=0.19\n",
            "  Downloading mne-0.24.1-py3-none-any.whl (7.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4 MB 39.5 MB/s \n",
            "\u001b[?25hCollecting coverage<6.0,>=5.5\n",
            "  Downloading coverage-5.5-cp37-cp37m-manylinux2010_x86_64.whl (242 kB)\n",
            "\u001b[K     |████████████████████████████████| 242 kB 48.3 MB/s \n",
            "\u001b[?25hCollecting PyYAML<6.0,>=5.0\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 40.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (3.1.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.19.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.19.5)\n",
            "Requirement already satisfied: matplotlib<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (3.2.2)\n",
            "Collecting scipy<2.0,>=1.5\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 29.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from moabb) (1.3.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.15.1 in /usr/local/lib/python3.7/dist-packages (from moabb) (2.23.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py<4.0,>=3.0->moabb) (1.5.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->moabb) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->moabb) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->moabb) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4.0,>=3.0->moabb) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0->moabb) (2018.9)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch<2.0,>=1.3->moabb) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pooch<2.0,>=1.3->moabb) (21.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyriemann>=0.2.6->moabb) (1.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib<4.0,>=3.0->moabb) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.15.1->moabb) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0,>=1.0->moabb) (3.1.0)\n",
            "Building wheels for collected packages: pyriemann\n",
            "  Building wheel for pyriemann (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyriemann: filename=pyriemann-0.2.7-py2.py3-none-any.whl size=49770 sha256=f231920f504ab34a4704a1f99c5109fc56dae42123634d454987373979410088\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/b7/55/27dcb08ed8fb58da8c1be108c23928ffb9125c9c1da2ddfb53\n",
            "Successfully built pyriemann\n",
            "Installing collected packages: scipy, PyYAML, pyriemann, mne, coverage, moabb\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: coverage\n",
            "    Found existing installation: coverage 3.7.1\n",
            "    Uninstalling coverage-3.7.1:\n",
            "      Successfully uninstalled coverage-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires coverage==3.7.1, but you have coverage 5.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "coveralls 0.5 requires coverage<3.999,>=3.6, but you have coverage 5.5 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-5.4.1 coverage-5.5 mne-0.24.1 moabb-0.4.4 pyriemann-0.2.7 scipy-1.7.3\n",
            "Collecting braindecode\n",
            "  Downloading Braindecode-0.6-py3-none-any.whl (177 kB)\n",
            "\u001b[K     |████████████████████████████████| 177 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from braindecode) (3.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from braindecode) (3.1.0)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.7/dist-packages (from braindecode) (0.24.1)\n",
            "Collecting skorch\n",
            "  Downloading skorch-0.11.0-py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 47.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from braindecode) (1.7.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->braindecode) (1.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->braindecode) (3.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->braindecode) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->braindecode) (2018.9)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from skorch->braindecode) (0.8.9)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from skorch->braindecode) (1.0.2)\n",
            "Requirement already satisfied: tqdm>=4.14.0 in /usr/local/lib/python3.7/dist-packages (from skorch->braindecode) (4.62.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch->braindecode) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->skorch->braindecode) (3.1.0)\n",
            "Installing collected packages: skorch, braindecode\n",
            "Successfully installed braindecode-0.6 skorch-0.11.0\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.12.10-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.1)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.26-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 47.4 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.5-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=83e4760417812772ec1505af8edc2316c031850d46fafe1376a12a8bd7cf0c23\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.26 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.5 shortuuid-1.0.8 smmap-5.0.0 wandb-0.12.10 yaspin-2.1.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzbBg4I9PAt_",
        "outputId": "53d212fa-b411-4c67-f700-0a1c795d9800"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/braindecode/datautil/windowers.py:4: UserWarning: datautil.windowers module is deprecated and is now under preprocessing.windowers, please use from import braindecode.preprocessing.windowers\n",
            "  warn('datautil.windowers module is deprecated and is now under '\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from braindecode.datasets.moabb import MOABBDataset\n",
        "from braindecode.datautil.windowers import create_windows_from_events\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.linalg import sqrtm, inv \n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Apply Euclidean Alignment\n",
        "def apply_EA(data):\n",
        "    '''\n",
        "    Apply Euclidean aligment on array-like objects for 1 subject\n",
        "    \n",
        "    PARAMETER:\n",
        "    data: \n",
        "        Data of one subject.\n",
        "    \n",
        "    \n",
        "    OUTPUT:\n",
        "        Aligned data with Euclidean Alignment\n",
        "    '''\n",
        "    \n",
        "    # So that this function can handles separated or combined left and right trials\n",
        "    # If they are separated\n",
        "\n",
        "    # If they are not separated\n",
        "\n",
        "    print('Found %d trial(s) in which EEG data is stored' %len(data))\n",
        "    all_trials = data\n",
        "    \n",
        "    # Calculate reference matrix\n",
        "    RefEA = 0\n",
        "    print('Computing reference matrix RefEA')\n",
        "\n",
        "    # Iterate over all trials, compute reference EA\n",
        "    for trial in all_trials:\n",
        "        cov = np.cov(trial, rowvar=True)\n",
        "        RefEA += cov\n",
        "\n",
        "    # Average over all trials\n",
        "    RefEA = RefEA/all_trials.shape[0]\n",
        "    \n",
        "    # Adding reference EA as a new key in data\n",
        "    data_dict={}\n",
        "    print('Add RefEA as a new key in data')\n",
        "    data_dict['RefEA'] = RefEA \n",
        "    \n",
        "    # Compute R^(-0.5)\n",
        "    R_inv = sqrtm(inv(RefEA))\n",
        "    data_dict['R_inv'] = R_inv\n",
        "    \n",
        "        \n",
        "    # Perform EA on each trial\n",
        "    all_trials_EA = []\n",
        "        \n",
        "    for t in all_trials:\n",
        "        all_trials_EA.append(R_inv@t)\n",
        "        \n",
        "    # Return all_trials_EA\n",
        "    return np.array(all_trials_EA)\n",
        "        \n",
        "\n",
        "######################## Load all data ################################\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, X, labels=None, transforms=None):\n",
        "        self.X = X\n",
        "        self.y = labels\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.X))\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        data = self.X[i,:,:]\n",
        "\n",
        "        if self.transforms:\n",
        "            data = self.transforms(data)\n",
        "\n",
        "        if self.y is not None:\n",
        "            return (data, self.y[i])\n",
        "        else:\n",
        "            return data\n",
        "    ############################################################################\n",
        "class TrainObject(object):\n",
        "    def __init__(self, X, y, euclidean_alignment=True):\n",
        "        assert len(X) == len(y)\n",
        "        if euclidean_alignment:\n",
        "            X=apply_EA(X)\n",
        "        mean = np.mean(X, axis=2, keepdims=True)\n",
        "        # Here standardize across the window, when channel size is not large enough\n",
        "        # In motor imagery kit, we put axis = 1, across channel as an example\n",
        "        std = np.std(X, axis=2, keepdims=True)\n",
        "        X = (X - mean) / std\n",
        "        # we scale it to 1000 as a better training scale of the shallow CNN\n",
        "        # according to the orignal work of the paper referenced above\n",
        "        self.X = X*1e3\n",
        "        self.y = y\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######################## Load all data ################################\n",
        "\n",
        "def load_data(loso): \n",
        "\n",
        "    ######################## Load all data ################################\n",
        "    T_x=[]\n",
        "    T_y=[]\n",
        "    V_x=[]\n",
        "    V_y=[]\n",
        "    Test_x=[]\n",
        "    Test_y=[]\n",
        "\n",
        "    for subject_id in [e for e in range(1,10) if e not in (loso, loso-1)]:\n",
        "        dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])\n",
        "\n",
        "\n",
        "        trial_start_offset_seconds = -0.5\n",
        "        # Extract sampling frequency, check that they are same in all datasets\n",
        "        sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "        assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
        "        # Calculate the trial start offset in samples.\n",
        "        trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
        "\n",
        "        # Create windows using braindecode function for this. It needs parameters to define how\n",
        "        # trials should be used.\n",
        "        windows_dataset = create_windows_from_events(\n",
        "            dataset,\n",
        "            trial_start_offset_samples=trial_start_offset_samples,\n",
        "            trial_stop_offset_samples=0,\n",
        "            preload=True,\n",
        "        )\n",
        "\n",
        "\n",
        "        splitted = windows_dataset.split('session')\n",
        "        train_set = splitted['session_T']\n",
        "        valid_set = splitted['session_E']\n",
        "\n",
        "        train_x=np.array([ele[0][:-1] for ele in train_set])\n",
        "        train_y=np.array([ele[1] for ele in train_set])\n",
        "\n",
        "        valid_x=np.array([ele[0][:-1] for ele in valid_set])\n",
        "        valid_y=np.array([ele[1] for ele in valid_set])\n",
        "\n",
        "        train_set = TrainObject(train_x, y=train_y)\n",
        "        valid_set = TrainObject(valid_x, y=valid_y)\n",
        "\n",
        "        [T_x.append(el) for el in train_set.X]\n",
        "        [T_y.append(el) for el in train_set.y]\n",
        "        [T_x.append(el) for el in valid_set.X]\n",
        "        [T_y.append(el) for el in valid_set.y]\n",
        "    T_x=np.array(T_x)\n",
        "    T_y=np.array(T_y)\n",
        "\n",
        "    ##################### Validation Set ###################################\n",
        "\n",
        "\n",
        "    subject_id = loso-1\n",
        "    dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])\n",
        "\n",
        "\n",
        "    trial_start_offset_seconds = -0.5\n",
        "    # Extract sampling frequency, check that they are same in all datasets\n",
        "    sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "    assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
        "    # Calculate the trial start offset in samples.\n",
        "    trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
        "\n",
        "    # Create windows using braindecode function for this. It needs parameters to define how\n",
        "    # trials should be used.\n",
        "    windows_dataset = create_windows_from_events(\n",
        "        dataset,\n",
        "        trial_start_offset_samples=trial_start_offset_samples,\n",
        "        trial_stop_offset_samples=0,\n",
        "        preload=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    splitted = windows_dataset.split('session')\n",
        "    train_set = splitted['session_T']\n",
        "    valid_set = splitted['session_E']\n",
        "\n",
        "    train_x=np.array([ele[0][:-1] for ele in train_set])\n",
        "    train_y=np.array([ele[1] for ele in train_set])\n",
        "\n",
        "    valid_x=np.array([ele[0][:-1] for ele in valid_set])\n",
        "    valid_y=np.array([ele[1] for ele in valid_set])\n",
        "        \n",
        "    train_set = TrainObject(train_x, y=train_y)\n",
        "    valid_set = TrainObject(valid_x, y=valid_y)\n",
        "\n",
        "    [V_x.append(el) for el in train_set.X]\n",
        "    [V_y.append(el) for el in train_set.y]\n",
        "    [V_x.append(el) for el in valid_set.X]\n",
        "    [V_y.append(el) for el in valid_set.y]\n",
        "\n",
        "    V_x=np.array(V_x)\n",
        "    V_y=np.array(V_y)\n",
        "\n",
        "\n",
        "    ############################### Test Set #############################################\n",
        "\n",
        "    subject_id = loso\n",
        "    dataset = MOABBDataset(dataset_name=\"BNCI2014001\", subject_ids=[subject_id])\n",
        "\n",
        "\n",
        "    trial_start_offset_seconds = -0.5\n",
        "    # Extract sampling frequency, check that they are same in all datasets\n",
        "    sfreq = dataset.datasets[0].raw.info['sfreq']\n",
        "    assert all([ds.raw.info['sfreq'] == sfreq for ds in dataset.datasets])\n",
        "    # Calculate the trial start offset in samples.\n",
        "    trial_start_offset_samples = int(trial_start_offset_seconds * sfreq)\n",
        "\n",
        "    # Create windows using braindecode function for this. It needs parameters to define how\n",
        "    # trials should be used.\n",
        "    windows_dataset = create_windows_from_events(\n",
        "        dataset,\n",
        "        trial_start_offset_samples=trial_start_offset_samples,\n",
        "        trial_stop_offset_samples=0,\n",
        "        preload=True,\n",
        "    )\n",
        "\n",
        "\n",
        "    splitted = windows_dataset.split('session')\n",
        "    train_set = splitted['session_T']\n",
        "    valid_set = splitted['session_E']\n",
        "\n",
        "    train_x=np.array([ele[0][:-1] for ele in train_set])\n",
        "    train_y=np.array([ele[1] for ele in train_set])\n",
        "\n",
        "    valid_x=np.array([ele[0][:-1] for ele in valid_set])\n",
        "    valid_y=np.array([ele[1] for ele in valid_set])\n",
        "\n",
        "    train_set = TrainObject(train_x, y=train_y)\n",
        "    valid_set = TrainObject(valid_x, y=valid_y)\n",
        "\n",
        "    [Test_x.append(el) for el in train_set.X]\n",
        "    [Test_y.append(el) for el in train_set.y]\n",
        "    [Test_x.append(el) for el in valid_set.X]\n",
        "    [Test_y.append(el) for el in valid_set.y]\n",
        "\n",
        "    Test_x=np.array(Test_x)\n",
        "    Test_y=np.array(Test_y)\n",
        "\n",
        "\n",
        "    ############################################################################\n",
        "    ##  LOSO  : 9 total subjects = 7 for training - 1 for validation - 1 for testing \n",
        "    return(T_x,T_y,V_x,V_y,Test_x,Test_y)\n"
      ],
      "metadata": {
        "id": "case6jDFPqMS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from math import ceil\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import init\n",
        "from torch.nn.utils import weight_norm\n",
        "from braindecode.models.modules import Expression, Ensure4d\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "def count_parameters(model):\n",
        "    table = PrettyTable([\"Modules\", \"Parameters\"])\n",
        "    total_params = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        if not parameter.requires_grad: continue\n",
        "        param = parameter.numel()\n",
        "        table.add_row([name, param])\n",
        "        total_params+=param\n",
        "    print(table)\n",
        "    print(f\"Total Trainable Params: {total_params}\")\n",
        "    return total_params   \n",
        "\n",
        "class PrintLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PrintLayer, self).__init__()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Do your print / debug stuff here\n",
        "        print(x.shape)\n",
        "        return x\n",
        "\n",
        "\n",
        "class _BatchNormZG(nn.BatchNorm2d):\n",
        "    def reset_parameters(self):\n",
        "        if self.track_running_stats:\n",
        "            self.running_mean.zero_()\n",
        "            self.running_var.fill_(1)\n",
        "        if self.affine:\n",
        "            self.weight.data.zero_()\n",
        "            self.bias.data.zero_()\n",
        "\n",
        "\n",
        "class _ConvBlock2D(nn.Module):\n",
        "    \"\"\"Implements Convolution block with order:\n",
        "    Convolution, dropout, activation, batch-norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_filters, out_filters, kernel, stride=(1, 1), padding=0, dilation=1,\n",
        "                 groups=1, drop_prob=0.5, batch_norm=True, activation=nn.LeakyReLU, residual=False):\n",
        "        super().__init__()\n",
        "        self.kernel = kernel\n",
        "        self.activation = activation()\n",
        "        self.residual = residual\n",
        "\n",
        "        self.conv = nn.Conv2d(in_filters, out_filters, kernel, stride=stride, padding=padding,\n",
        "                              dilation=dilation, groups=groups, bias=not batch_norm)\n",
        "        self.dropout = nn.Dropout2d(p=drop_prob)\n",
        "        self.batch_norm = _BatchNormZG(out_filters) if residual else nn.BatchNorm2d(out_filters) if\\\n",
        "            batch_norm else lambda x: x\n",
        "\n",
        "    def forward(self, input):\n",
        "        res = input\n",
        "        input = self.conv(input,)\n",
        "        input = self.dropout(input)\n",
        "        input = self.activation(input)\n",
        "        input = self.batch_norm(input)\n",
        "        return input + res if self.residual else input\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "class _DenseFilter(nn.Module):\n",
        "    def __init__(self, in_features, growth_rate, filter_len=5, drop_prob=0.5, bottleneck=2,\n",
        "                 activation=nn.LeakyReLU, dim=-2):\n",
        "        super().__init__()\n",
        "        dim = dim if dim > 0 else dim + 4\n",
        "        if dim < 2 or dim > 3:\n",
        "            raise ValueError('Only last two dimensions supported')\n",
        "        kernel = (filter_len, 1) if dim == 2 else (1, filter_len)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_features),\n",
        "            activation(),\n",
        "            nn.Conv2d(in_features, bottleneck * growth_rate, 1),\n",
        "            nn.BatchNorm2d(bottleneck * growth_rate),\n",
        "            activation(),\n",
        "            nn.Conv2d(bottleneck * growth_rate, growth_rate, kernel,\n",
        "                      padding=tuple((k // 2 for k in kernel))),\n",
        "            nn.Dropout2d(drop_prob)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat((x, self.net(x)), dim=1)\n",
        "\n",
        "\n",
        "class _DenseSpatialFilter(nn.Module):\n",
        "    def __init__(self, in_chans, growth, depth, in_ch=1, bottleneck=4, drop_prob=0.0,\n",
        "                 activation=nn.LeakyReLU, collapse=True):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(*[\n",
        "            _DenseFilter(in_ch + growth * d, growth, bottleneck=bottleneck, drop_prob=drop_prob,\n",
        "                         activation=activation) for d in range(depth)\n",
        "        ])\n",
        "        n_filters = in_ch + growth * depth\n",
        "        self.collapse = collapse\n",
        "        if collapse:\n",
        "            self.channel_collapse = _ConvBlock2D(n_filters, n_filters, (in_chans, 1), drop_prob=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if len(x.shape) < 4:\n",
        "            x = x.unsqueeze(1).permute([0, 1, 3, 2])\n",
        "        x = self.net(x)\n",
        "        if self.collapse:\n",
        "            return self.channel_collapse(x).squeeze(-2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class _TemporalFilter(nn.Module):\n",
        "    def __init__(self, in_chans, filters, depth, temp_len, drop_prob=0., activation=nn.LeakyReLU,\n",
        "                 residual='netwise'):\n",
        "        super().__init__()\n",
        "        temp_len = temp_len + 1 - temp_len % 2\n",
        "        self.residual_style = str(residual)\n",
        "        net = list()\n",
        "\n",
        "        for i in range(depth):\n",
        "            dil = depth - i\n",
        "            conv = weight_norm(nn.Conv2d(in_chans if i == 0 else filters, filters,\n",
        "                                         kernel_size=(1, temp_len), dilation=dil,\n",
        "                                         padding=(0, dil * (temp_len - 1) // 2)))\n",
        "            net.append(nn.Sequential(\n",
        "                conv,\n",
        "                activation(),\n",
        "                nn.Dropout2d(drop_prob)\n",
        "            ))\n",
        "        if self.residual_style.lower() == 'netwise':\n",
        "            self.net = nn.Sequential(*net)\n",
        "            self.residual = nn.Conv2d(in_chans, filters, (1, 1))\n",
        "        elif residual.lower() == 'dense':\n",
        "            self.net = net\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.residual_style.lower() == 'netwise':\n",
        "            return self.net(x) + self.residual(x)\n",
        "        elif self.residual_style.lower() == 'dense':\n",
        "            for layer in self.net:\n",
        "                x = torch.cat((x, layer(x)), dim=1)\n",
        "            return x\n",
        "\n",
        "\n",
        "class _TIDNetFeatures(nn.Module):\n",
        "    def __init__(self,  oo, loop ,s_growth, t_filters, in_chans, input_window_samples, drop_prob, pooling,\n",
        "                 temp_layers, spat_layers, temp_span, bottleneck, summary):\n",
        "        super().__init__()\n",
        "        self.in_chans = in_chans\n",
        "        self.input_windows_samples = input_window_samples\n",
        "        self.temp_len = ceil(temp_span * input_window_samples)\n",
        "\n",
        "        def _permute(x):\n",
        "            \"\"\"\n",
        "            Permutes data:\n",
        "            from dim:\n",
        "            batch, chans, time, 1\n",
        "            to dim:\n",
        "            batch, 1, chans, time\n",
        "            \"\"\"\n",
        "            return x.permute([0, 3, 1, 2])\n",
        "\n",
        "        self.temporal = nn.Sequential(\n",
        "            Ensure4d(),\n",
        "            Expression(_permute),\n",
        "            _TemporalFilter(1, t_filters, depth=temp_layers, temp_len=self.temp_len),\n",
        "            nn.MaxPool2d((1, pooling)),\n",
        "            nn.Dropout2d(drop_prob),\n",
        "        )\n",
        "        summary = input_window_samples // pooling if summary == -1 else summary\n",
        "\n",
        "        self.spatial = _DenseSpatialFilter(in_chans, s_growth, spat_layers, in_ch=t_filters,\n",
        "                                           drop_prob=drop_prob, bottleneck=bottleneck)\n",
        "        self.extract_features = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool1d(int(summary)),\n",
        "            nn.Flatten(start_dim=1)\n",
        "        )\n",
        "\n",
        "        self._num_features = (t_filters + s_growth * spat_layers) * summary\n",
        "\n",
        "        \n",
        "        self.loop=loop\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.enc0 = nn.Conv2d(32, oo, (1,1))\n",
        "        self.enc1 = nn.Conv2d(oo, oo, (1,1))    \n",
        "\n",
        "        ### compute input of fc after flatten\n",
        "        #after enc0\n",
        "        h= ((25-1)/1)+1  \n",
        "        w= ((75-1)/1)+1  \n",
        "        h=int(  (h-2)/2 +1 )\n",
        "        w=int( (w-2)/2 +1 )\n",
        "\n",
        "        #after enc1 loop\n",
        "        for i in range(self.loop):\n",
        "            h=((h-1)/1)+1  \n",
        "            w= ((w-1)/1)+1  \n",
        "            h= int( (h-2)/2 +1 )\n",
        "            w=int( (w-2)/2 +1 )\n",
        "        ##################\n",
        "\n",
        "          \n",
        "        self.fc_block2 = nn.Linear(oo*h*w, 4)\n",
        "\n",
        "    @property\n",
        "    def num_features(self):\n",
        "        return self._num_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.temporal(x)\n",
        "        #x = self.spatial(x)\n",
        "        #return self.extract_features(x)\n",
        "\n",
        "        x=F.relu(self.pool(self.enc0(x)))\n",
        "\n",
        "        for i in range(self.loop):\n",
        "            x=F.relu(self.pool(self.enc1(x)))\n",
        "\n",
        "        x = torch.flatten(x, 1)\n",
        "        out = self.fc_block2(x)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "########################################### \n",
        "\n",
        "class TIDNet_features(nn.Module):\n",
        "\n",
        "    def __init__(self, oo, loop, in_chans, n_classes, input_window_samples, s_growth=24, t_filters=32,\n",
        "                 drop_prob=0.4, pooling=15, temp_layers=2, spat_layers=2, temp_span=0.05,\n",
        "                 bottleneck=3, summary=-1):\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.in_chans = in_chans\n",
        "        self.input_window_samples = input_window_samples\n",
        "        self.temp_len = ceil(temp_span * input_window_samples)\n",
        "\n",
        "        self.dscnn = _TIDNetFeatures( oo, loop,s_growth=s_growth, t_filters=t_filters, in_chans=in_chans,\n",
        "                                     input_window_samples=input_window_samples,\n",
        "                                     drop_prob=drop_prob, pooling=pooling, temp_layers=temp_layers,\n",
        "                                     spat_layers=spat_layers, temp_span=temp_span,\n",
        "                                     bottleneck=bottleneck, summary=summary)\n",
        "\n",
        "        self._num_features = self.dscnn.num_features\n",
        "\n",
        "        self.classify = self._create_classifier(self.num_features, n_classes)\n",
        "\n",
        "    def _create_classifier(self, incoming, n_classes):\n",
        "        classifier = nn.Linear(incoming, n_classes)\n",
        "        init.xavier_normal_(classifier.weight)\n",
        "        classifier.bias.data.zero_()\n",
        "        return nn.Sequential(nn.Flatten(start_dim=1), classifier, nn.LogSoftmax(dim=-1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"Forward pass.\n",
        "        Parameters\n",
        "        ----------\n",
        "        x: torch.Tensor\n",
        "            Batch of EEG windows of shape (batch_size, n_channels, n_times).\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.dscnn(x)\n",
        "        return x #self.classify(x)\n",
        "    \n",
        "    def get_emb(self, x):\n",
        "        return self.dscnn(x)\n",
        "\n",
        "    @property\n",
        "    def num_features(self):\n",
        "        return self._num_features\n",
        "############################################################################\n"
      ],
      "metadata": {
        "id": "HBE9QQz_Qvms"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################################################################\n",
        "\n",
        "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
        "device = 'cuda' if cuda else 'cpu'\n",
        "\n",
        "############################################################################\n",
        "\n",
        "\n",
        "def build_network(oo,loop):\n",
        "\n",
        "    model=TIDNet_features(oo,loop, n_classes=4, in_chans=25, input_window_samples=1125, s_growth=24, t_filters=32,\n",
        "                 drop_prob=0.4, pooling=15, temp_layers=2, spat_layers=2, temp_span=0.05,\n",
        "                 bottleneck=3, summary=-1)\n",
        "    return model.to(device)\n",
        "\n",
        "\n",
        "def build_optimizer(network, optimizer, learning_rate):\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = optim.SGD(network.parameters(),\n",
        "                              lr=learning_rate, momentum=0.9, weight_decay=0.5*0.001)\n",
        "    elif optimizer == \"adamw\":\n",
        "        optimizer = optim.AdamW(network.parameters(),\n",
        "                               lr=learning_rate,  weight_decay=0.01, amsgrad=True)\n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def train_epoch(network, loader, optimizer, loss_config, batch_size, tr_iter):\n",
        "    cumu_loss = 0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "    \n",
        "    for i, (data, target) in tqdm(enumerate(loader), ncols = 100, total=68,\n",
        "               desc =\"Training\"):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        if data.shape[0]==batch_size:\n",
        "\n",
        "            data.double()\n",
        "            target.long()\n",
        "            optimizer.zero_grad()\n",
        "            network.double()\n",
        "\n",
        "\n",
        "\n",
        "            # ➡ Forward pass\n",
        "            if loss_config == \"nll_loss\":\n",
        "                #loss = floss.forward(network(data.double()), target.long())\n",
        "                loss = F.nll_loss(network(data.double()), target.long())\n",
        "                cumu_loss += loss.item()\n",
        "            elif loss_config =='CrossEntropyLoss':\n",
        "                loss = F.cross_entropy(network(data.double()), target.long())\n",
        "                cumu_loss += loss.item()\n",
        "        \n",
        "            # ⬅ Backward pass + weight update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "             # compute accuracy\n",
        "            outputs = network(data.double())\n",
        "\n",
        "            # Get predictions from the maximum value\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Total number of labels\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum()\n",
        "\n",
        "\n",
        "            tr_iter=tr_iter+1\n",
        "\n",
        "    return cumu_loss / len(loader), correct/total, tr_iter\n",
        "\n",
        "\n",
        "def validate_epoch(network, loader, optimizer, loss_config, batch_size, v_itr):\n",
        "    cumu_loss = 0.0\n",
        "    correct = 0.0\n",
        "    total = 0.0\n",
        "   \n",
        "    for _, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        if data.shape[0]==batch_size:\n",
        "            data.double()\n",
        "            target.long()\n",
        "            \n",
        "        \n",
        "            optimizer.zero_grad()\n",
        "            network.double()\n",
        "            network.eval()  \n",
        "            torch.no_grad()\n",
        "            \n",
        "            # Compute loss\n",
        "            if loss_config == \"nll_loss\":\n",
        "                #loss = F.nll_loss(network(data.double()), target.long())\n",
        "                cumu_loss += loss.item()\n",
        "            elif loss_config =='CrossEntropyLoss':\n",
        "                loss = F.cross_entropy(network(data.double()), target.long())\n",
        "                cumu_loss += loss.item()      \n",
        "\n",
        "          # compute accuracy\n",
        "            outputs = network(data.double())\n",
        "\n",
        "            # Get predictions from the maximum value\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Total number of labels\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum()   \n",
        "\n",
        "            v_itr=v_itr+1\n",
        "    return cumu_loss / len(loader), correct/total, v_itr\n",
        "\n",
        "\n",
        "def test(network, loader, batch_size, n_classes, t_itr):\n",
        "    # Calculate Accuracy\n",
        "    correct = 0.0\n",
        "    correct_arr = [0.0] * n_classes\n",
        "    total = 0.0\n",
        "    total_arr = [0.0] * n_classes\n",
        "    y_true=[]\n",
        "    y_pred=[]\n",
        "    # Iterate through test dataset\n",
        "    for data, target in loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        if data.shape[0] == batch_size:  #condition to avoid taking trials length < batch size (problematic for confusion matrix)\n",
        "            target.long()\n",
        "            network.double()\n",
        "            outputs = network(data.double())\n",
        "                    # Get predictions from the maximum value\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # Total number of labels\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum()\n",
        "            y_true.append(target)\n",
        "            y_pred.append(predicted)\n",
        "           \n",
        "            for label in range(n_classes):\n",
        "                correct_arr[label] += (((predicted == target) & (target==label)).sum())\n",
        "                total_arr[label] += (target == label).sum()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print('TEST ACCURACY {} '.format(accuracy))\n",
        "    \n",
        "    #print('TEST F1-Score {} '.format(f1_score(torch.tensor(np.array(y_true,'int32')).view(-1), torch.tensor(np.array(y_pred,'int32')).view(-1),  average='macro')))\n",
        "    t_itr=t_itr+1               \n",
        "    return accuracy, t_itr\n",
        "\n",
        "import wandb\n",
        "\n",
        "project_name=\"sweep_encoder_f_test\"\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'grid', #grid\n",
        "    'metric': {\n",
        "      'name': 'loss',\n",
        "      'goal': 'minimize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "        'epochs': {\n",
        "            'values': [9]\n",
        "        },\n",
        "        'batch_size': {\n",
        "            'values': [60]\n",
        "        },\n",
        "     \n",
        "        'learning_rate': {\n",
        "            'values': [1e-3]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adamw']\n",
        "        },\n",
        "         'loss': {\n",
        "            'values': ['CrossEntropyLoss'],\n",
        "        },\n",
        "        'subject': {\n",
        "            'values': [3,4,5,6,7,8,9]\n",
        "        },\n",
        "        'number_of_layers': {\n",
        "            'values': [1,2,3,4]\n",
        "        },\n",
        "        'number_of_filters': {\n",
        "            'values': [1,5,32,64]\n",
        "        },\n",
        "      \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=project_name)\n",
        "\n",
        "def train_wandb():\n",
        "    # Initialize a new wandb run\n",
        "    with wandb.init(project=project_name, #entity=\"brain-imt\", \n",
        "    config=sweep_config):\n",
        "        config = wandb.config\n",
        "\n",
        "        n_classes=4\n",
        "        batch_size=config.batch_size\n",
        "        LR=config.learning_rate\n",
        "        optim= config.optimizer\n",
        "        loss=config.loss\n",
        "\n",
        "\n",
        "        network = build_network(1,1)#config.number_of_filters, config.number_of_layers)\n",
        "        \n",
        "        pytorch_total_params = sum(p.numel() for p in network.parameters())\n",
        "        print(pytorch_total_params)\n",
        "        #wandb.log({\"total number of parameters\": pytorch_total_params})\n",
        "        optimizer = build_optimizer(network, optim, LR)\n",
        "\n",
        "        T_x,T_y, V_x,V_y,Test_x,Test_y=load_data(5)#config.subject)\n",
        "\n",
        "        tr_x=T_x\n",
        "        tr_y=T_y\n",
        "\n",
        "\n",
        "        val_x=V_x\n",
        "        val_y=V_y\n",
        "\n",
        "\n",
        "        test_x=Test_x\n",
        "        test_y=Test_y\n",
        "\n",
        "        train_data = EEGDataset(tr_x, tr_y, transforms=None)\n",
        "        valid_data = EEGDataset(val_x, val_y, transforms=None)\n",
        "        test_data = EEGDataset(test_x, test_y, transforms=None)\n",
        "\n",
        "        del tr_x, tr_y, val_x, val_y, test_x, test_y\n",
        "\n",
        "\n",
        "\n",
        "        train_loader = DataLoader(train_data, batch_size=batch_size,shuffle=True)\n",
        "        valid_loader = DataLoader(valid_data, batch_size=batch_size,shuffle=True)\n",
        "        test_loader = DataLoader(test_data, batch_size=batch_size,shuffle=True)\n",
        "\n",
        "        Train_acc=[]\n",
        "        Val_acc=[]\n",
        "        Train_loss=[]\n",
        "        Val_loss=[]\n",
        "        Test_acc=[]\n",
        "    ## Tensorboard iterators\n",
        "        tr_iter=0\n",
        "        v_itr=0\n",
        "        maxv=0\n",
        "\n",
        "        summary(network.cuda(), (25, 1125))\n",
        "\n",
        "\n",
        "        for epoch in range(config.epochs):\n",
        "\n",
        "            train_loss,train_acc,tr_iter = train_epoch(network, train_loader, optimizer, loss, batch_size, tr_iter)\n",
        "            print('train loss {} accuracy {} epoch {} done'.format(train_loss,train_acc,epoch))\n",
        "            val_loss,val_acc,v_itr = validate_epoch(network, valid_loader, optimizer, loss, batch_size, v_itr)\n",
        "            print('val loss {} epoch {} done'.format(val_loss,epoch))\n",
        "            Train_acc.append(train_acc)\n",
        "            Val_acc.append(val_acc)\n",
        "            Train_loss.append(train_loss)\n",
        "            Val_loss.append(val_loss)\n",
        "            wandb.log({'Training accuracy': train_acc, 'Training loss': train_loss})\n",
        "            wandb.log({'Validation accuracy': val_acc, 'Validation loss': val_loss})\n",
        "            if epoch % 3 == 0:\n",
        "                test_acc,_=test(network, test_loader, batch_size, n_classes, epoch)\n",
        "                Test_acc.append(test_acc)\n",
        "                wandb.log({'Test accuracy': test_acc})\n",
        "            if maxv<val_acc:\n",
        "                wandb.log({'Maximum validation accuracy': val_acc})\n",
        "                maxv=val_acc\n",
        "            \n",
        "\n",
        "\n",
        "        \n",
        "        '''x_np=[]\n",
        "        y_np=[]\n",
        "        for data, target in train_loader:\n",
        "            network.to('cpu')\n",
        "            x_np.append(network.get_emb(data).detach().numpy().tolist())\n",
        "            y_np.append(target)\n",
        "\n",
        "        v=[]\n",
        "        for i in x_np :\n",
        "            for j in i:\n",
        "                v.append(np.array(j))  \n",
        "        vv=[]\n",
        "        for i in y_np :\n",
        "            for j in i:\n",
        "                vv.append(np.array(j))\n",
        "        print('X shape  ',np.array(v).shape)\n",
        "        print('Y shape  ',np.array(vv).shape)\n",
        "        np.savez_compressed('train.npz',x=np.array(v),y=np.array(vv))'''\n",
        "\n",
        "        return Train_acc, Val_acc, Test_acc\n",
        "\n",
        "    ############################################################################\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLsQHSHeQWev",
        "outputId": "30035467-af88-4c5f-94b8-614a08d6be75"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: twd32prx\n",
            "Sweep URL: https://wandb.ai/ghblg/sweep_encoder_f_test/sweeps/twd32prx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#os.environ[\"WANDB_MODE\"]=\"offline\"\n",
        "\n",
        "wandb.agent(sweep_id, train_wandb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RggUariuQy57",
        "outputId": "b324fb4a-f447-4c39-a663-9fbb2275b60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 45eevl5w with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: CrossEntropyLoss\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_filters: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnumber_of_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adamw\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsubject: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mghblg\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/ghblg/sweep_encoder_f_test/runs/45eevl5w\" target=\"_blank\">brisk-sweep-1</a></strong> to <a href=\"https://wandb.ai/ghblg/sweep_encoder_f_test\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "Sweep page: <a href=\"https://wandb.ai/ghblg/sweep_encoder_f_test/sweeps/twd32prx\" target=\"_blank\">https://wandb.ai/ghblg/sweep_encoder_f_test/sweeps/twd32prx</a><br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/moabb/datasets/download.py:53: RuntimeWarning: Setting non-standard config type: \"MNE_DATASETS_BNCI_PATH\"\n",
            "  set_config(key, get_config(\"MNE_DATA\"))\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A01T.mat'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "269291\n",
            "MNE_DATA is not already configured. It will be set to default location in the home directory - /root/mne_data\n",
            "All datasets will be downloaded to this location, if anything is already downloaded, please move manually to this location\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "SHA256 hash of downloaded file: 054f02e70cf9c4ada1517e9b9864f45407939c1062c6793516585c6f511d0325\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A01E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A01E.mat'.\n",
            "SHA256 hash of downloaded file: 53d415f39c3d7b0c88b894d7b08d99bcdfe855ede63831d3691af1a45607fb62\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A02T.mat'.\n",
            "SHA256 hash of downloaded file: 5ddd5cb520b1692c3ba1363f48d98f58f0e46f3699ee50d749947950fc39db27\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A02E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A02E.mat'.\n",
            "SHA256 hash of downloaded file: d63c454005d3a9b41d8440629482e855afc823339bdd0b5721842a7ee9cc7b12\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A03T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A03T.mat'.\n",
            "SHA256 hash of downloaded file: 7e731ee8b681d5da6ecb11ae1d4e64b1653c7f15aad5d6b7620b25ce53141e80\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A03E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A03E.mat'.\n",
            "SHA256 hash of downloaded file: d4229267ec7624fa8bd3af5cbebac17f415f7c722de6cb676748f8cb3b717d97\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A06T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A06T.mat'.\n",
            "SHA256 hash of downloaded file: 4dc3be1b0d60279134d1220323c73c68cf73799339a7fb224087a3c560a9a7e2\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A06E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A06E.mat'.\n",
            "SHA256 hash of downloaded file: bf67a40621b74b6af7a986c2f6edfff7fc2bbbca237aadd07b575893032998d1\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A07T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A07T.mat'.\n",
            "SHA256 hash of downloaded file: 43b6bbef0be78f0ac2b66cb2d9679091f1f5b7f0a5d4ebef73d2c7cc8e11aa96\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A07E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A07E.mat'.\n",
            "SHA256 hash of downloaded file: b9aaec73dcee002fab84ee98e938039a67bf6a3cbf4fc86d5d8df198cfe4c323\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A08T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A08T.mat'.\n",
            "SHA256 hash of downloaded file: 7a4b3bd602d5bc307d3f4527fca2cf076659e94aca584dd64f6286fd413a82f2\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A08E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A08E.mat'.\n",
            "SHA256 hash of downloaded file: 0eedbd89790c7d621c8eef68065ddecf80d437bbbcf60321d9253e2305f294f7\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A09T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A09T.mat'.\n",
            "SHA256 hash of downloaded file: b28d8a262c779c8cad9cc80ee6aa9c5691cfa6617c03befe490a090347ebd15c\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A09E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A09E.mat'.\n",
            "SHA256 hash of downloaded file: 5d79649a42df9d51215def8ffbdaf1c3f76c54b88b9bbaae721e8c6fd972cc36\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A04T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A04T.mat'.\n",
            "SHA256 hash of downloaded file: 15850d81b95fc88cc8b9589eb9b713d49fa071e28adaf32d675b3eaa30591d6e\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A04E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A04E.mat'.\n",
            "SHA256 hash of downloaded file: 81916dff2c12997974ba50ffc311da006ea66e525010d010765f0047e771c86a\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A05T.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A05T.mat'.\n",
            "SHA256 hash of downloaded file: 77387d3b669f4ed9a7c1dac4dcba4c2c40c8910bae20fb961bb7cf5a94912950\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n",
            "Downloading data from 'http://bnci-horizon-2020.eu/database/data-sets/001-2014/A05E.mat' to file '/root/mne_data/MNE-bnci-data/database/data-sets/001-2014/A05E.mat'.\n",
            "SHA256 hash of downloaded file: 8b357470865610c28b2f1d351beac247a56a856f02b2859d650736eb2ef77808\n",
            "Use this value as the 'known_hash' argument of 'pooch.retrieve' to ensure that the file hasn't changed if it is downloaded again in the future.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "48 events found\n",
            "Event IDs: [1 2 3 4]\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Used Annotations descriptions: ['feet', 'left_hand', 'right_hand', 'tongue']\n",
            "Adding metadata with 4 columns\n",
            "Replacing existing metadata with 4 columns\n",
            "48 matching events found\n",
            "No baseline correction applied\n",
            "0 projection items activated\n",
            "Loading data for 48 events and 1125 original time points ...\n",
            "0 bad epochs dropped\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "Found 288 trial(s) in which EEG data is stored\n",
            "Computing reference matrix RefEA\n",
            "Add RefEA as a new key in data\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "          Ensure4d-1          [-1, 25, 1125, 1]               0\n",
            "        Expression-2          [-1, 1, 25, 1125]               0\n",
            "            Conv2d-3         [-1, 32, 25, 1125]           1,856\n",
            "         LeakyReLU-4         [-1, 32, 25, 1125]               0\n",
            "         Dropout2d-5         [-1, 32, 25, 1125]               0\n",
            "            Conv2d-6         [-1, 32, 25, 1125]          58,400\n",
            "         LeakyReLU-7         [-1, 32, 25, 1125]               0\n",
            "         Dropout2d-8         [-1, 32, 25, 1125]               0\n",
            "            Conv2d-9         [-1, 32, 25, 1125]              64\n",
            "  _TemporalFilter-10         [-1, 32, 25, 1125]               0\n",
            "        MaxPool2d-11           [-1, 32, 25, 75]               0\n",
            "        Dropout2d-12           [-1, 32, 25, 75]               0\n",
            "           Conv2d-13            [-1, 1, 25, 75]              33\n",
            "        MaxPool2d-14            [-1, 1, 12, 37]               0\n",
            "           Conv2d-15            [-1, 1, 12, 37]               2\n",
            "        MaxPool2d-16             [-1, 1, 6, 18]               0\n",
            "           Linear-17                    [-1, 4]             436\n",
            "  _TIDNetFeatures-18                    [-1, 4]               0\n",
            "================================================================\n",
            "Total params: 60,791\n",
            "Trainable params: 60,791\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.11\n",
            "Forward/backward pass size (MB): 56.30\n",
            "Params size (MB): 0.23\n",
            "Estimated Total Size (MB): 56.64\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|█████████████████████████████████████████████████████| 68/68 [02:23<00:00,  2.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 1.3671447039096092 accuracy 0.2497512549161911 epoch 0 done\n",
            "val loss 1.248501957565023 epoch 0 done\n",
            "TEST ACCURACY 0.2518518567085266 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|█████████████████████████████████████████████████████| 68/68 [02:22<00:00,  2.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 1.3666763270446074 accuracy 0.2492537498474121 epoch 1 done\n",
            "val loss 1.247765044170562 epoch 1 done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  71%|█████████████████████████████████████▍               | 48/68 [01:40<00:42,  2.14s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rx48s_CeZHre"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}